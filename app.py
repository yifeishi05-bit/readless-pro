# ReadLess Pro â€” æ— æ¨¡å‹å¤§ PDF ç¨³å®šæ‘˜è¦ç‰ˆï¼ˆè¶…å¤§æ–‡ä»¶å‹å¥½ï¼‰
import io
import math
import re
from collections import Counter, defaultdict
from typing import List, Tuple

import streamlit as st
import pdfplumber

st.set_page_config(page_title="ğŸ“˜ ReadLess Pro (Model-free)", page_icon="ğŸ“˜", layout="wide")
st.title("ğŸ“š ReadLess Pro â€” å¤§ PDF ç¨³å®šæ‘˜è¦ï¼ˆæ— æ¨¡å‹ï¼‰")
st.caption("ä¸ä¾èµ–ä»»ä½•å¤§æ¨¡å‹ï¼›é’ˆå¯¹ 500~1000 é¡µé•¿æ–‡æ¡£åšæå–å¼æ‘˜è¦ã€‚è‹¥æ˜¯æ‰«æä»¶è¯·å…ˆåš OCRã€‚")

# ---------------- å·¥å…·å‡½æ•° ----------------
_SENT_SPLIT = re.compile(r"(?<=[ã€‚ï¼ï¼Ÿ!?ï¼.])\s+|(?<=[;ï¼›])\s+|(?<=[\n])")
_WORD_SPLIT = re.compile(r"[^\w\u4e00-\u9fff]+")

STOPWORDS = set("""
çš„ äº† å’Œ ä¸ åŠ è€Œ ä¸” åœ¨ ä¸º å¯¹ ä»¥ å¹¶ å°† æŠŠ è¢« è¿™ é‚£ å…¶ ä¹‹ äº ä» åˆ° ç­‰ ç­‰ç­‰
æ˜¯ å°± éƒ½ åˆ å¾ˆ åŠå…¶ æ¯” è¾ƒ æ›´ æœ€ å„ ä¸ª å·² å·²ç» å¦‚æœ å› ä¸º æ‰€ä»¥ ä½†æ˜¯ ç„¶è€Œ
æˆ‘ä»¬ ä½ ä»¬ ä»–ä»¬ å¥¹ä»¬ å®ƒä»¬ æœ¬ æ–‡ ä¹‹ä¸€ å…¶ä¸­ é€šè¿‡ è¿›è¡Œ èƒ½å¤Ÿ å¯ä»¥
a an the and or but if then else for to of in on with as by from into over under
be is are was were been being this that these those it its their our your
""".split())

def split_sentences(text: str) -> List[str]:
    # å…ˆç»Ÿä¸€ç©ºç™½
    text = re.sub(r"[ \t]+", " ", text)
    text = re.sub(r"\n{2,}", "\n", text)
    parts = _SENT_SPLIT.split(text)
    sents = []
    for s in parts:
        s2 = s.strip()
        if len(s2) >= 2:
            sents.append(s2)
    return sents

def words_in(s: str) -> List[str]:
    # ä¸­è‹±æ··åˆåˆ†è¯ï¼ˆæç®€ï¼‰ï¼šä¸­æ–‡æŒ‰å­—è¯æ··åˆã€è‹±æ–‡æŒ‰ \w
    tokens = []
    for t in _WORD_SPLIT.split(s):
        t = t.strip().lower()
        if not t:
            continue
        # å•ä¸ªæ±‰å­—ä¹Ÿä¿ç•™ï¼Œè‹±æ–‡å»åœç”¨è¯
        if (len(t) == 1 and not re.match(r"[\u4e00-\u9fff]", t)):
            continue
        if t in STOPWORDS:
            continue
        tokens.append(t)
    return tokens

def summarize_extractive(text: str, max_sent: int = 6) -> str:
    sents = split_sentences(text)
    if not sents:
        return ""
    # è®¡ç®—è¯é¢‘ & å¥å­åˆ†æ•°ï¼ˆTF * IDF-è¿‘ä¼¼ï¼‰
    docs = [words_in(s) for s in sents]
    df = Counter()
    for dw in docs:
        for w in set(dw):
            df[w] += 1
    N = len(sents)
    scores = []
    for i, dw in enumerate(docs):
        if not dw:
            scores.append((0.0, i)); continue
        tf = Counter(dw)
        s = 0.0
        for w, c in tf.items():
            idf = math.log(1 + N / (1 + df[w]))
            s += (c / len(dw)) * idf
        # è½»åº¦é•¿åº¦æƒ©ç½šï¼Œé¿å…è¶…é•¿å¥ç‹¬éœ¸
        s = s / (1.0 + 0.15 * max(0, len(dw) - 40))
        scores.append((s, i))
    # é€‰ top-kï¼ŒæŒ‰åŸæ–‡é¡ºåºè¿˜åŸï¼Œå¢å¼ºå¯è¯»æ€§
    k = max(3, min(max_sent, max(3, int(N * 0.1))))
    top_idx = [i for _, i in sorted(scores, key=lambda x: x[0], reverse=True)[:k]]
    top_idx.sort()
    return " ".join(sents[i] for i in top_idx)

def chunk_pages(pages_text: List[str], pages_per_chunk: int) -> List[Tuple[int, int, str]]:
    chunks = []
    for i in range(0, len(pages_text), pages_per_chunk):
        j = min(len(pages_text), i + pages_per_chunk)
        t = "\n".join(pages_text[i:j]).strip()
        if t:
            chunks.append((i+1, j, t))  # (èµ·å§‹é¡µ, ç»“æŸé¡µ, æ–‡æœ¬)
    return chunks

# ---------------- ä¾§è¾¹æ å‚æ•° ----------------
with st.sidebar:
    st.header("âš™ï¸ è®¾ç½®")
    pages_per_chunk = st.slider("æ¯æ®µåˆå¹¶é¡µæ•°", 10, 50, 20, 2, help="æŒ‰é¡µåˆå¹¶ååšåˆ†æ®µæ‘˜è¦ï¼Œæå‡ç¨³å®šæ€§ä¸é€Ÿåº¦")
    summary_sents = st.slider("æ¯æ®µæ‘˜è¦å¥æ•°ï¼ˆä¸Šé™ï¼‰", 4, 12, 6, 1)
    final_sents = st.slider("æ€»æ‘˜è¦å¥æ•°ï¼ˆä¸Šé™ï¼‰", 6, 20, 12, 1)
    hard_cap_chars = st.number_input("å•æ®µå­—ç¬¦ç¡¬ä¸Šé™", min_value=10000, value=25000, step=5000,
                                     help="é˜²æ­¢è¶…é•¿æ®µå¯¼è‡´å†…å­˜/æ—¶é—´æš´æ¶¨ï¼›è¶…å‡ºå³æˆªæ–­")
    st.caption("æç¤ºï¼šè‹¥ PDF æ˜¯æ‰«æä»¶ï¼ˆæ— å¯æŠ½å–æ–‡æœ¬ï¼‰ï¼Œä¼šæç¤ºéœ€å…ˆ OCRã€‚")

# ---------------- ä¸»æµç¨‹ ----------------
uploaded = st.file_uploader("ğŸ“„ ä¸Šä¼  PDFï¼ˆæ”¯æŒè¶…é•¿æ–‡æ¡£ï¼‰", type="pdf")
if not uploaded:
    st.stop()

st.info("æ–‡ä»¶å·²ä¸Šä¼ ï¼Œå¼€å§‹è§£æé¡µé¢æ–‡æœ¬â€¦")
raw = uploaded.read()

pages_text = []
try:
    with pdfplumber.open(io.BytesIO(raw)) as pdf:
        total = len(pdf.pages)
        st.write(f"æ£€æµ‹åˆ°é¡µæ•°ï¼š**{total}**")
        prog = st.progress(0.0)
        for i, page in enumerate(pdf.pages, start=1):
            try:
                t = page.extract_text(x_tolerance=2, y_tolerance=2) or ""
            except Exception:
                t = ""
            t = t.strip()
            if t:
                # ç®€å•æ¸…æ´— & é™åˆ¶å•é¡µé•¿åº¦ï¼Œé¿å…æç«¯é¡µé¢
                t = re.sub(r"[ \t]+", " ", t)
                if len(t) > 12000:
                    t = t[:12000]
                pages_text.append(t)
            if i % 10 == 0 or i == total:
                prog.progress(i / total)
except Exception as e:
    st.error(f"âŒ è§£æ PDF å¤±è´¥ï¼š{e}")
    st.stop()

if not pages_text:
    st.error("âŒ æœªæŠ½å–åˆ°æ­£æ–‡æ–‡æœ¬ã€‚è¿™é€šå¸¸æ˜¯æ‰«æç‰ˆ/å›¾ç‰‡ç‰ˆ PDFã€‚è¯·å…ˆç”¨ OCRï¼ˆå¦‚ Google Drive OCRã€Adobeã€ABBYYï¼‰è½¬æˆå¯å¤åˆ¶æ–‡æœ¬çš„ PDFã€‚")
    st.stop()

# åˆ†æ®µ
chunks = chunk_pages(pages_text, pages_per_chunk)
st.write(f"ğŸ” æŒ‰æ¯ {pages_per_chunk} é¡µåˆå¹¶ï¼Œå…±å¾—åˆ° **{len(chunks)}** æ®µã€‚")
summs = []
prog2 = st.progress(0.0)

for idx, (p_from, p_to, text) in enumerate(chunks, start=1):
    if len(text) > hard_cap_chars:
        text = text[:hard_cap_chars]
    s = summarize_extractive(text, max_sent=summary_sents) or "(æœ¬æ®µå†…å®¹è¿‡äºç¨€ç–ï¼Œæœªç”Ÿæˆæ‘˜è¦)"
    summs.append(f"### ğŸ“– ç¬¬ {idx} æ®µï¼ˆé¡µ {p_from}â€“{p_to}ï¼‰\n{s}")
    prog2.progress(idx / len(chunks))

st.success("âœ… åˆ†æ®µæ‘˜è¦å®Œæˆï¼")
for s in summs:
    st.markdown(s)

# æœ€ç»ˆæ€»æ‘˜è¦ï¼ˆå¯¹æ‰€æœ‰åˆ†æ®µæ‘˜è¦å†æ¬¡åšæå–å¼æ±‡æ€»ï¼‰
st.divider()
st.subheader("ğŸ“™ å…¨ä¹¦æ‘˜è¦ï¼ˆæå–å¼ï¼‰")
joined = " ".join(s.replace("### ", "").replace("\n", " ") for s in summs)
final_sum = summarize_extractive(joined, max_sent=final_sents) or "(æ€»æ‘˜è¦ç”Ÿæˆå¤±è´¥â€”â€”åŸæ–‡å¯èƒ½è¿‡çŸ­)"
st.write(final_sum)

# å¯¼å‡º
st.download_button(
    label="â¬‡ï¸ ä¸‹è½½æ‘˜è¦ Markdown",
    data=("\n\n".join(summs) + "\n\n---\n\n## å…¨ä¹¦æ‘˜è¦\n" + final_sum).encode("utf-8"),
    file_name="summary.md",
    mime="text/markdown"
)

st.caption("ğŸš€ æ¨¡å‹è‡ªç”±ï¼ˆæ—  Torch/Transformersï¼‰Â· é•¿æ–‡ç¨³å®š Â· è¿›åº¦å¯è§† Â· é€‚åˆæ•™æ/è®²ä¹‰/æŠ¥å‘Š")

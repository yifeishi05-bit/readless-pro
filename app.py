# ReadLess Pro â€” åˆ†å±‚æ‘˜è¦ï¼ˆ20é¡µä¸€æ®µ Ã— äºŒçº§æ±‡æ€»ï¼‰ã€ç¨³å®šå¯è·‘ç‰ˆã€‘
# çº¯ Python / ä¸ç”¨ transformers / ä¸ç”¨ torchï¼Œé€‚åˆå¤§ PDF

import io
import re
import time
import textwrap
from collections import Counter
from typing import List, Tuple

import streamlit as st
import pdfplumber

# ---------------- UI ----------------
st.set_page_config(page_title="ğŸ“˜ ReadLess Pro â€” åˆ†å±‚æ‘˜è¦ï¼ˆç¨³å®šå¯è·‘ç‰ˆï¼‰", page_icon="ğŸ“˜", layout="wide")
st.title("ğŸ“š ReadLess Pro â€” åˆ†å±‚æ‘˜è¦ï¼ˆç¨³å®šå¯è·‘ç‰ˆï¼‰")
st.caption("ä¸Šä¼ ä»»æ„å¤§çš„**æ–‡æœ¬å‹**PDFï¼šæ¯20é¡µæ‘˜è¦â†’äºŒçº§æ±‡æ€»â†’æœ€ç»ˆæ€»è§ˆï¼ˆå‚æ•°å¯è°ƒï¼‰ã€‚")

with st.sidebar:
    st.header("âš™ï¸ å‚æ•°")
    pages_per_chunk = st.number_input("æ¯æ®µåŒ…å«çš„é¡µæ•°ï¼ˆä¸€çº§ï¼‰", 5, 50, 20, 1)
    sents_per_chunk = st.number_input("æ¯æ®µä¿ç•™å¥æ•°ï¼ˆä¸€çº§ï¼‰", 3, 20, 6, 1)
    chunks_per_super = st.number_input("å¤šå°‘æ®µåˆå¹¶ä¸ºä¸€ç»„ï¼ˆäºŒçº§ï¼‰", 5, 50, 20, 1)
    sents_per_super = st.number_input("æ¯ç»„ä¿ç•™å¥æ•°ï¼ˆäºŒçº§ï¼‰", 3, 30, 8, 1)
    sents_final = st.number_input("æœ€ç»ˆæ€»è§ˆå¥æ•°", 5, 40, 14, 1)

    st.divider()
    hard_caps = st.toggle("å¼€å¯é•¿åº¦é™å¹…ï¼ˆæ›´ç¨³ï¼‰", value=True)
    debug = st.toggle("æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯", value=False)

    st.caption("æç¤ºï¼šæ‰«æ/å›¾ç‰‡å‹PDFæ— æ³•ç›´æ¥æå–æ–‡å­—ï¼Œæœ¬å·¥å…·ä¼šæç¤ºå…ˆåšOCRã€‚")

uploaded = st.file_uploader("ğŸ“„ ä¸Šä¼  PDFï¼ˆä¹¦ç±/æŠ¥å‘Š/è®²ä¹‰ï¼Œéæ‰«æï¼‰", type=["pdf"])
if not uploaded:
    st.stop()

# ---------------- å·¥å…·å‡½æ•° ----------------
_CJK_RANGE = (
    ("\u4e00", "\u9fff"),  # CJK Unified Ideographs
    ("\u3400", "\u4dbf"),  # CJK Extension A
)

def is_cjk(ch: str) -> bool:
    if len(ch) != 1:
        return False
    o = ord(ch)
    for a, b in _CJK_RANGE:
        if ord(a) <= o <= ord(b):
            return True
    return False

def split_sentences(text: str) -> List[str]:
    # ä¸­è‹±æ–‡é€šç”¨å¥å­åˆ‡åˆ†
    text = re.sub(r"[ \t]+", " ", text)
    # å…ˆå¤„ç†ä¸­æ–‡æ ‡ç‚¹ï¼Œå†å¤„ç†è‹±æ–‡
    parts = re.split(r"(?<=[ã€‚ï¼ï¼Ÿâ€¦])\s*|\s*(?<=[!?])\s+", text)
    sents = [s.strip() for s in parts if s and s.strip()]
    return sents

_EN_STOP = set("""
a an the of to in for on with at from into during including until against among throughout despite toward upon
I you he she it we they me him her them my your his their our ours yours mine
and or but if while though although as than so because since unless until whereas whether nor
is am are was were be being been do does did done doing have has had having can could may might must shall should will would
""".split())
# å¸¸è§ä¸­æ–‡è™šè¯/åœç”¨è¯ï¼ˆç®€ç‰ˆï¼‰
_ZH_STOP = set(list("çš„äº†å‘¢å§å•Šå˜›å“¦å‘€å‘€ç€è¿‡ä¹Ÿå¾ˆéƒ½å°±å¹¶è€ŒåŠä¸æŠŠè¢«å¯¹äºä¸æ˜¯æ²¡æœ‰è¿˜æ˜¯"))

def tokenize(text: str) -> List[str]:
    # å¯¹ä¸­æ–‡ï¼šæŒ‰å•å­—ï¼ˆå»åœç”¨ï¼‰ï¼›å¯¹è‹±æ–‡ï¼š\w+ å°å†™ï¼ˆå»åœç”¨ï¼‰
    if any(is_cjk(c) for c in text):
        toks = [c for c in text if is_cjk(c) and c not in _ZH_STOP]
    else:
        toks = [w.lower() for w in re.findall(r"[A-Za-z0-9]+", text)]
        toks = [w for w in toks if w not in _EN_STOP and len(w) > 1]
    return toks

def summarize_extractive(text: str, keep: int, cap_chars: int = 40000) -> str:
    """
    çº¯æŠ½å–å¼æ‘˜è¦ï¼ˆé¢‘æ¬¡ + ä½ç½®å¾®è°ƒï¼‰
    - cap_chars: æˆªæ–­ä¸Šé™é˜²çˆ†å†…å­˜/è¶…é•¿
    """
    if not text.strip():
        return ""

    if hard_caps and len(text) > cap_chars:
        text = text[:cap_chars]

    sents = split_sentences(text)
    if not sents:
        return ""

    # ç»Ÿè®¡è¯é¢‘
    freq = Counter()
    for s in sents:
        for t in tokenize(s):
            freq[t] += 1
    if not freq:
        # æ²¡æ³•ç»Ÿè®¡å°±å–å¼€å¤´è‹¥å¹²å¥
        return " ".join(sents[:keep])

    maxf = max(freq.values())
    for k in list(freq.keys()):
        freq[k] = freq[k] / maxf

    # å¥å­æ‰“åˆ†ï¼šè¯é¢‘å’Œ + ä½ç½®å¥–åŠ±ï¼ˆé å‰ç•¥é«˜ï¼‰
    scored: List[Tuple[int, float, str]] = []
    n = len(sents)
    for i, s in enumerate(sents):
        tokens = tokenize(s)
        base = sum(freq.get(t, 0) for t in tokens)
        # ä½ç½®å¥–åŠ±ï¼šå‰ 20% ç¨å¾®åŠ åˆ†
        pos_bonus = 0.15 if i < max(1, int(0.2 * n)) else 0.0
        length_norm = (len(tokens) ** 0.5) or 1.0
        score = (base / length_norm) + pos_bonus
        scored.append((i, score, s))

    # é€‰ topKï¼Œä½†ä¿æŒåŸé¡ºåº
    scored.sort(key=lambda x: x[1], reverse=True)
    top = sorted(scored[:max(1, keep)], key=lambda x: x[0])
    return " ".join(s for (_, _, s) in top)

def chunk_pages_text(pages: List[str], group: int) -> List[str]:
    chunks = []
    for i in range(0, len(pages), group):
        part = "\n".join(pages[i:i+group])
        chunks.append(part)
    return chunks

def safe_extract_text(file_bytes: bytes) -> Tuple[List[str], List[int]]:
    pages_text, empty_pages = [], []
    with pdfplumber.open(io.BytesIO(file_bytes)) as pdf:
        total = len(pdf.pages)
        pbar = st.progress(0.0, text="è§£æPDFé¡µé¢ä¸­â€¦")
        for i, page in enumerate(pdf.pages, start=1):
            try:
                t = page.extract_text(x_tolerance=2, y_tolerance=2) or ""
            except Exception:
                t = ""
            if not t.strip():
                empty_pages.append(i)
            pages_text.append(t)
            if i % 5 == 0 or i == total:
                pbar.progress(i / total, text=f"è§£æPDFé¡µé¢ä¸­â€¦ï¼ˆ{i}/{total}ï¼‰")
    return pages_text, empty_pages

# ---------------- ä¸»æµç¨‹ ----------------
try:
    raw = uploaded.read()
except Exception as e:
    st.error(f"è¯»å–æ–‡ä»¶å¤±è´¥ï¼š{e}")
    st.stop()

t0 = time.time()
pages_text, empty_pages = safe_extract_text(raw)
total_pages = len(pages_text)
st.success(f"âœ… å·²è§£æé¡µæ•°ï¼š{total_pages} é¡µ")
if empty_pages:
    st.warning(f"æœ‰ {len(empty_pages)} é¡µå‡ ä¹æ²¡æœ‰å¯è¯»æ–‡å­—ï¼ˆå¯èƒ½æ˜¯æ‰«æ/å›¾ç‰‡é¡µï¼‰ã€‚ç¤ºä¾‹ï¼š{empty_pages[:10]}â€¦")

# ä¸€çº§ï¼šæ¯ N é¡µä¸€æ®µ â†’ æŠ½å–è‹¥å¹²å¥
level1_chunks = chunk_pages_text(pages_text, pages_per_chunk)
st.write(f"ğŸ”¹ ä¸€çº§åˆ†æ®µæ•°ï¼š**{len(level1_chunks)}**ï¼ˆæ¯æ®µçº¦ {pages_per_chunk} é¡µï¼‰")

l1_summaries: List[str] = []
pb1 = st.progress(0.0, text="ä¸€çº§æ‘˜è¦ç”Ÿæˆä¸­â€¦")
for i, chunk in enumerate(level1_chunks, start=1):
    try:
        summ = summarize_extractive(chunk, keep=sents_per_chunk, cap_chars=60000)
    except Exception as e:
        summ = f"(ç¬¬ {i} æ®µæ‘˜è¦å¤±è´¥ï¼š{e})"
    l1_summaries.append(summ)
    if i % 2 == 0 or i == len(level1_chunks):
        pb1.progress(i / len(level1_chunks), text=f"ä¸€çº§æ‘˜è¦ç”Ÿæˆä¸­â€¦ï¼ˆ{i}/{len(level1_chunks)}ï¼‰")

st.subheader("ğŸ“– ä¸€çº§æ‘˜è¦ï¼ˆæŒ‰æ®µï¼‰")
for idx, s in enumerate(l1_summaries, start=1):
    st.markdown(f"**æ®µ {idx}**ï¼š{s}")

# äºŒçº§ï¼šæ¯ M æ®µåˆå¹¶ â†’ å†æŠ½å–
l2_inputs = chunk_pages_text(l1_summaries, chunks_per_super)
st.write(f"ğŸ”¹ äºŒçº§æ±‡æ€»ç»„æ•°ï¼š**{len(l2_inputs)}**ï¼ˆæ¯ç»„ {chunks_per_super} æ®µï¼‰")

l2_summaries: List[str] = []
pb2 = st.progress(0.0, text="äºŒçº§æ±‡æ€»ç”Ÿæˆä¸­â€¦")
for i, group_text in enumerate(l2_inputs, start=1):
    try:
        summ = summarize_extractive(group_text, keep=sents_per_super, cap_chars=40000)
    except Exception as e:
        summ = f"(ç¬¬ {i} ç»„æ±‡æ€»å¤±è´¥ï¼š{e})"
    l2_summaries.append(summ)
    if i % 1 == 0 or i == len(l2_inputs):
        pb2.progress(i / len(l2_inputs), text=f"äºŒçº§æ±‡æ€»ç”Ÿæˆä¸­â€¦ï¼ˆ{i}/{len(l2_inputs)}ï¼‰")

st.subheader("ğŸ“š äºŒçº§æ±‡æ€»ï¼ˆæŒ‰ç»„ï¼‰")
for idx, s in enumerate(l2_summaries, start=1):
    st.markdown(f"**ç»„ {idx}**ï¼š{s}")

# æœ€ç»ˆæ€»è§ˆ
st.subheader("ğŸ§­ æœ€ç»ˆæ€»è§ˆ")
final_input = "\n".join(l2_summaries) if l2_summaries else "\n".join(l1_summaries)
final_summary = summarize_extractive(final_input, keep=sents_final, cap_chars=50000)
st.write(final_summary)

# å¯¼å‡º
st.divider()
export_txt = []
export_txt.append("# æœ€ç»ˆæ€»è§ˆ\n" + textwrap.fill(final_summary, width=100))
export_txt.append("\n# äºŒçº§æ±‡æ€»\n" + "\n\n".join(f"ã€ç»„{i+1}ã€‘{s}" for i, s in enumerate(l2_summaries)))
export_txt.append("\n# ä¸€çº§æ‘˜è¦\n" + "\n\n".join(f"ã€æ®µ{i+1}ã€‘{s}" for i, s in enumerate(l1_summaries)))
txt_bytes = "\n\n".join(export_txt).encode("utf-8", errors="ignore")
st.download_button("ğŸ“¥ ä¸‹è½½æ‘˜è¦ï¼ˆTXTï¼‰", data=txt_bytes, file_name="readless_summary.txt", mime="text/plain")

# è°ƒè¯•ä¿¡æ¯
if debug:
    st.divider()
    st.caption(f"â±ï¸ ç”¨æ—¶ï¼š{time.time()-t0:.2f}s | é¡µæ•°ï¼š{total_pages} | ä¸€çº§æ®µæ•°ï¼š{len(level1_chunks)} | äºŒçº§ç»„æ•°ï¼š{len(l2_inputs)}")
    st.caption(f"å‚æ•°ï¼špages_per_chunk={pages_per_chunk}, sents_per_chunk={sents_per_chunk}, chunks_per_super={chunks_per_super}, sents_per_super={sents_per_super}, sents_final={sents_final}")

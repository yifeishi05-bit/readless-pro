import os
import re
import jieba
import streamlit as st
import pdfplumber
from collections import Counter

# -------------------- é¡µé¢é…ç½® --------------------
st.set_page_config(page_title="ReadLess Pro â€“ Lite (No-Model)", page_icon="ğŸ“˜")

# -------------------- é—¨ç¦é€»è¾‘ --------------------
REAL_CODE = os.getenv("ACCESS_CODE") or st.secrets.get("ACCESS_CODE", "")
BUY_LINK = "https://readlesspro.lemonsqueezy.com/buy/d0a09dc2-f156-4b4b-8407-12a87943bbb6"

st.sidebar.title("ğŸ”’ Member Login")
code = st.sidebar.text_input("Enter access code (for paid users)", type="password")
if code != REAL_CODE:
    st.warning("Please enter a valid access code to continue.")
    st.markdown(f"ğŸ’³ [Click here to subscribe ReadLess Pro]({BUY_LINK})")
    st.stop()

# -------------------- å·¥å…·å‡½æ•°ï¼šè¯­è¨€æ£€æµ‹ä¸åˆ†å¥ --------------------
CJK_RE = re.compile(r"[\u4e00-\u9fff]")
EN_SENT_SPLIT_RE = re.compile(r"(?<=[.!?])\s+")
CN_SENT_SPLIT_RE = re.compile(r"[ã€‚ï¼ï¼Ÿâ€¦]+")

EN_STOPWORDS = {
    "the","a","an","is","are","was","were","am","be","to","of","and","in","that",
    "for","on","at","by","with","as","it","its","this","these","those","from",
    "or","not","but","than","then","so","such","if","into","their","there","they",
    "you","your","we","our","i","me","my","he","she","his","her","them","us",
}

CN_STOPWORDS = {
    "çš„","äº†","å’Œ","ä¸","åŠ","ä¹Ÿ","å¹¶","æˆ–","è€Œ","è¢«","å¯¹","åœ¨","æ˜¯","ä¸º","äº","åŠå…¶",
    "ä¸€ä¸ª","ä¸€äº›","æˆ‘ä»¬","ä½ ä»¬","ä»–ä»¬","å®ƒä»¬","ä»¥åŠ","åŒæ—¶","å› æ­¤","ä½†æ˜¯","å¦‚æœ",
}

def is_chinese(text: str) -> bool:
    if not text:
        return False
    cjk = len(CJK_RE.findall(text))
    return (cjk / max(1, len(text))) > 0.2

def split_sentences(text: str, lang_cn: bool):
    if lang_cn:
        # ä¿ç•™åˆ†éš”ç¬¦ä½œä¸ºå¥æœ«æ ‡è®°
        parts = [s.strip() for s in CN_SENT_SPLIT_RE.split(text) if s.strip()]
        ends  = CN_SENT_SPLIT_RE.findall(text)
        sents = []
        for i, p in enumerate(parts):
            end = ends[i] if i < len(ends) else ""
            sents.append((p + (end if end else "")))
        return sents
    else:
        return [s.strip() for s in EN_SENT_SPLIT_RE.split(text) if s.strip()]

# -------------------- è¯é¢‘ä¸å¥å­æ‰“åˆ† --------------------
def word_tokens(text: str, lang_cn: bool):
    if lang_cn:
        # ä¸­æ–‡ç”¨ç»“å·´åˆ†è¯ï¼Œå»æ‰æ ‡ç‚¹å’Œåœç”¨è¯
        tokens = [t.strip() for t in jieba.cut(text) if t.strip()]
        tokens = [t for t in tokens if t not in CN_STOPWORDS and not re.fullmatch(r"\W+", t)]
        return tokens
    else:
        tokens = re.findall(r"[A-Za-z]+", text.lower())
        tokens = [t for t in tokens if t not in EN_STOPWORDS and len(t) > 1]
        return tokens

def score_sentences(sentences, lang_cn: bool):
    # å»ºç«‹å…¨æ–‡è¯é¢‘
    all_tokens = []
    for s in sentences:
        all_tokens.extend(word_tokens(s, lang_cn))
    if not all_tokens:
        return [0.0]*len(sentences)

    freq = Counter(all_tokens)
    maxf = max(freq.values())
    # TFï¼ˆç®€åŒ–ï¼‰å½’ä¸€åŒ–
    for k in freq:
        freq[k] = freq[k]/maxf

    scores = []
    for s in sentences:
        toks = word_tokens(s, lang_cn)
        if not toks:
            scores.append(0.0)
            continue
        # å¥å­åˆ†æ•° = è¯é¢‘å’Œ / log(å¥é•¿+1)ï¼ˆé˜²æ­¢åè¢’è¶…é•¿å¥ï¼‰
        raw = sum(freq.get(t, 0.0) for t in toks)
        denom = max(1.0, (len(toks))**0.6)
        scores.append(raw/denom)
    return scores

def extractive_summary(text: str, max_sentences: int = 8) -> str:
    lang_cn = is_chinese(text)
    sentences = split_sentences(text, lang_cn)
    if not sentences:
        return "ï¼ˆæœªèƒ½ä»æ–‡æœ¬ä¸­è¯†åˆ«åˆ°æœ‰æ•ˆå¥å­ï¼‰"

    # å¤ªé•¿åˆ™ä»…å–å‰è‹¥å¹²å­—ç¬¦è¿›è¡Œæ‘˜è¦ä»¥ä¿è¯é€Ÿåº¦
    joined = "\n".join(sentences)
    if len(joined) > 20000:
        # å–å‰é¢éƒ¨åˆ†ï¼Œé€šå¸¸å·²è¦†ç›–ä¸»è¦ä¿¡æ¯
        head = int(len(joined)*0.6)
        text = joined[:head]
        sentences = split_sentences(text, lang_cn)

    scores = score_sentences(sentences, lang_cn)
    # é€‰å–å¾—åˆ†æœ€é«˜çš„ N ä¸ªå¥å­ï¼Œå¹¶æŒ‰åŸæ–‡é¡ºåºæ¢å¤
    idx_sorted = sorted(range(len(sentences)), key=lambda i: scores[i], reverse=True)
    chosen_idx = sorted(idx_sorted[:max_sentences])
    chosen = [sentences[i] for i in chosen_idx]

    # åˆå¹¶ï¼Œé¿å…è¿‡çŸ­æ‘˜è¦
    result = " ".join(chosen).strip()
    if not result:
        result = "ï¼ˆæ‘˜è¦ä¸ºç©ºï¼Œå¯èƒ½åŸæ–‡ä¸ºå›¾ç‰‡æˆ–ä¸å¯é€‰æ–‡æœ¬ï¼‰"
    return result

# -------------------- è¯»å– PDF æ–‡æœ¬ --------------------
def read_pdf_text(file) -> str:
    text = ""
    with pdfplumber.open(file) as pdf:
        for page in pdf.pages:
            t = page.extract_text()
            if t:
                text += t + "\n"
    return text

# -------------------- é¡µé¢ä¸»ä½“ --------------------
st.title("ğŸ“˜ ReadLess Pro â€“ Lite Edition (No Model, Cloud-safe)")
st.subheader("ä¸Šä¼  PDF æˆ–ç²˜è´´æ–‡æœ¬ï¼Œç”Ÿæˆå¿«é€Ÿæ‘˜è¦ï¼ˆæ— éœ€å¤§æ¨¡å‹ã€æ— éœ€ API Keyï¼‰")

tab1, tab2 = st.tabs(["ğŸ“„ ä¸Šä¼  PDF", "âœï¸ ç›´æ¥ç²˜è´´æ–‡æœ¬"])

with tab1:
    uploaded = st.file_uploader("é€‰æ‹© PDF æ–‡ä»¶", type="pdf")
    max_sent = st.slider("æ‘˜è¦å¥å­æ•°", 3, 12, 8)
    if uploaded is not None:
        with st.spinner("æ­£åœ¨è§£æ PDF æ–‡æœ¬â€¦"):
            content = read_pdf_text(uploaded)
        if not content.strip():
            st.error("âŒ æ²¡æœ‰ä» PDF ä¸­æå–åˆ°å¯ç”¨æ–‡æœ¬ï¼ˆå¯èƒ½æ˜¯æ‰«æä»¶æˆ–å›¾ç‰‡ï¼‰ã€‚")
        else:
            st.info("âœ… è§£æå®Œæˆï¼Œå¼€å§‹ç”Ÿæˆæ‘˜è¦â€¦")
            summary = extractive_summary(content, max_sentences=max_sent)
            st.success("ğŸ§  æ‘˜è¦ï¼š")
            st.write(summary)

with tab2:
    raw = st.text_area("æŠŠè¦æ‘˜è¦çš„å†…å®¹ç²˜è´´åˆ°è¿™é‡Œ", height=220, placeholder="æ”¯æŒä¸­è‹±æ··åˆæ–‡æœ¬")
    max_sent2 = st.slider("æ‘˜è¦å¥å­æ•°ï¼ˆæ–‡æœ¬æ¨¡å¼ï¼‰", 3, 12, 8, key="txt_slider")
    if st.button("ç”Ÿæˆæ‘˜è¦", type="primary"):
        if not raw.strip():
            st.warning("è¯·å…ˆç²˜è´´æ–‡æœ¬ã€‚")
        else:
            with st.spinner("æ­£åœ¨ç”Ÿæˆæ‘˜è¦â€¦"):
                summary = extractive_summary(raw, max_sentences=max_sent2)
            st.success("ğŸ§  æ‘˜è¦ï¼š")
            st.write(summary)

st.caption("âš¡ è½»é‡ç‰ˆï¼šä¸ä½¿ç”¨ transformers/torchï¼Œå®‰è£…è¿…é€Ÿã€äº‘ç«¯ç¨³å®šã€‚")

# app.py â€” ReadLess Pro (ONNX / CPU-only / Py3.13-safe)

import os
import io
import sys
import warnings
from typing import List

# â€”â€” é‡è¦ï¼šé¿å… transformers åœ¨å¯¼å…¥æ—¶æ¢æµ‹åˆ° torch â€”â€” #
os.environ["TRANSFORMERS_NO_TORCH"] = "1"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

import streamlit as st
import pdfplumber

warnings.filterwarnings("ignore", category=SyntaxWarning)

# ----------------- é¡µé¢ -----------------
st.set_page_config(page_title="ğŸ“˜ ReadLess Pro â€“ Book Summarizer", page_icon="ğŸ“˜", layout="wide")
st.title("ğŸ“š ReadLess Pro â€“ AI Book Summarizer")
st.caption("Upload a long PDF (even full books!) and get automatic chapter summaries powered by ONNX T5-small (no PyTorch).")

# ----------------- ä¼šå‘˜ -----------------
REAL_CODE = os.getenv("ACCESS_CODE") or st.secrets.get("ACCESS_CODE", "")
BUY_LINK = "https://readlesspro.lemonsqueezy.com/buy/d0a09dc2-f156-4b4b-8407-12a87943bbb6"

with st.sidebar:
    st.header("ğŸ”’ Member Login")
    code = st.text_input("Enter access code (for paid users)", type="password")
    st.markdown(f"è‹¥æ²¡æœ‰ï¼šğŸ’³ [ç‚¹å‡»è®¢é˜… ReadLess Pro]({BUY_LINK})")

    st.divider()
    st.header("âš™ï¸ Controls")
    mode = st.radio("Summary mode",
                    ["å¿«é€Ÿï¼ˆæœ€çŸ­ï¼‰", "æ ‡å‡†ï¼ˆæ¨èï¼‰", "è¯¦ç»†ï¼ˆæ›´é•¿ï¼‰"], index=1)
    presets = {
        "å¿«é€Ÿï¼ˆæœ€çŸ­ï¼‰":  dict(sections=16, sec_max=140, sec_min=60, final_max=260, final_min=120),
        "æ ‡å‡†ï¼ˆæ¨èï¼‰":  dict(sections=20, sec_max=180, sec_min=70, final_max=320, final_min=140),
        "è¯¦ç»†ï¼ˆæ›´é•¿ï¼‰":  dict(sections=26, sec_max=220, sec_min=90, final_max=420, final_min=200),
    }
    P = presets[mode]

    est_pages = st.number_input("ä¼°è®¡é¡µæ•°ï¼ˆå¯é€‰ï¼‰", min_value=1, value=200, step=50,
                                help="å¡«å†™åä¼šè‡ªåŠ¨è°ƒèŠ‚åˆ†æ®µæ•°ï¼Œæ›´è´´åˆä¹¦æœ¬é•¿åº¦")
    if est_pages:
        P["sections"] = min(40, max(10, int(est_pages / 18)))  # çº¦18é¡µä¸€æ®µï¼Œæ›´ä¿å®ˆ

    max_sections = P["sections"]
    per_section_max_len = P["sec_max"]
    per_section_min_len = P["sec_min"]
    final_max_len = P["final_max"]
    final_min_len = P["final_min"]

    with st.expander("é«˜çº§è®¾ç½®ï¼ˆå¯é€‰ï¼‰", expanded=False):
        max_sections   = st.number_input("Max sections to summarize", 5, 120, max_sections, 1)
        per_section_max_len = st.slider("Per-section max length", 80, 300, per_section_max_len, 10)
        per_section_min_len = st.slider("Per-section min length", 30, 200, per_section_min_len, 10)
        final_max_len  = st.slider("Final summary max length", 150, 500, final_max_len, 10)
        final_min_len  = st.slider("Final summary min length", 80, 300, final_min_len, 10)

    st.caption(f"Python: {sys.version.split()[0]}")

if code != REAL_CODE:
    st.warning("è¯·è¾“å…¥æœ‰æ•ˆçš„è®¿é—®ç ç»§ç»­ä½¿ç”¨ã€‚")
    st.stop()

# ----------------- æ‡’åŠ è½½ ONNX æ¨¡å‹ï¼ˆä¸å¯¼å…¥ torchï¼‰ -----------------
@st.cache_resource(show_spinner=True)
def load_onnx_summarizer():
    # åªåœ¨è¿™é‡Œå¯¼å…¥ transformers/optimumï¼Œé¿å…æ¨¡å—çº§å¯¼å…¥æ—¶çš„åç«¯æ¢æµ‹
    from transformers import AutoTokenizer, pipeline
    from optimum.onnxruntime import ORTModelForSeq2SeqLM

    model_id = "echarlaix/t5-small-onnx"  # å…¬å¼€çš„ T5-small ONNX æƒé‡ï¼ˆCPUï¼‰
    tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)
    tok.model_max_length = 512

    # æŒ‡å®š CPUExecutionProviderï¼Œå½»åº•è§„é¿ CUDA/torch ç›¸å…³è·¯å¾„
    model = ORTModelForSeq2SeqLM.from_pretrained(
        model_id,
        provider="CPUExecutionProvider",
        use_cache=False,
    )

    # transformers çš„ pipeline èƒ½ç›´æ¥å¥— ORT æ¨¡å‹
    summarizer = pipeline("summarization", model=model, tokenizer=tok)
    return summarizer, tok


# ----------------- Token çº§åˆ†å—ï¼ˆä¸¥æ ¼ï¼‰ -----------------
def chunk_by_tokens(tokenizer, text: str, max_tokens: int = 360, overlap: int = 32) -> List[str]:
    """
    ä¿å®ˆä¸Šé™ 360ï¼ˆ<<512ï¼‰ï¼Œå¹¶å¸¦é‡å ï¼›å½»åº•æœç» â€œtoken indices 766 > 512â€ ç±»é”™è¯¯ã€‚
    """
    if not text.strip():
        return []
    paras = [p.strip() for p in text.replace("\r\n", "\n").split("\n") if p.strip()]
    chunks, buf = [], []
    buf_ids_len = 0

    def ids_len(t: str) -> int:
        return len(tokenizer.encode(t, add_special_tokens=False))

    for p in paras:
        p_len = ids_len(p)
        if p_len > max_tokens:
            # ç”¨å¥å·ç­‰æ–­å¥ç¬¦è¿›ä¸€æ­¥ç»†åˆ†ï¼Œé¿å…ç²—æš´æˆªæ–­
            sents, tmp = [], []
            for seg in p.replace("ã€‚", "ã€‚|").replace("ï¼", "ï¼|").replace("ï¼Ÿ", "ï¼Ÿ|").split("|"):
                s = seg.strip()
                if s:
                    tmp.append(s)
                    if s[-1:] in "ã€‚ï¼ï¼Ÿ.!?":
                        sents.append("".join(tmp)); tmp = []
            if tmp: sents.append("".join(tmp))
            for s in sents:
                s_len = ids_len(s)
                if buf_ids_len + s_len <= max_tokens:
                    buf.append(s); buf_ids_len += s_len
                else:
                    if buf:
                        piece = " ".join(buf); chunks.append(piece)
                        tail = piece[-overlap * 2 :] if overlap > 0 else ""
                        buf = [tail] if tail else []
                        buf_ids_len = ids_len(" ".join(buf)) if buf else 0
                    if s_len <= max_tokens:
                        buf.append(s); buf_ids_len = ids_len(" ".join(buf))
                    else:
                        ids = tokenizer.encode(s, add_special_tokens=False)
                        for i in range(0, len(ids), max_tokens):
                            chunks.append(tokenizer.decode(ids[i:i+max_tokens]))
                        buf, buf_ids_len = [], 0
        else:
            if buf_ids_len + p_len <= max_tokens:
                buf.append(p); buf_ids_len += p_len
            else:
                piece = " ".join(buf); chunks.append(piece)
                tail = piece[-overlap * 2 :] if overlap > 0 else ""
                buf = [tail, p] if tail else [p]
                buf_ids_len = ids_len(" ".join(buf))
    if buf:
        chunks.append(" ".join(buf))
    return [c for c in chunks if c.strip()]


# ----------------- ä¸»é€»è¾‘ -----------------
def main():
    uploaded = st.file_uploader("ğŸ“„ Upload a PDF file (book, report, or notes)", type="pdf")
    if not uploaded:
        return

    st.info("âœ… File uploaded successfully. Extracting textâ€¦")
    text_parts: List[str] = []
    try:
        raw = uploaded.read()
        with pdfplumber.open(io.BytesIO(raw)) as pdf:
            total_pages = len(pdf.pages)
            st.write(f"Total pages detected: **{total_pages}**")
            progress_pages = st.progress(0.0)
            for i, page in enumerate(pdf.pages, start=1):
                try:
                    # æé«˜å®¹é”™ï¼šå®¹å·®ç¨å¾®æ”¾å®½
                    t = page.extract_text(x_tolerance=2, y_tolerance=2)
                except Exception:
                    t = ""
                if t:
                    text_parts.append(t)
                if i % 10 == 0 or i == total_pages:
                    progress_pages.progress(i / total_pages)
    except Exception as e:
        st.error(f"âŒ Failed to parse PDF: {e}")
        return

    full_text = "\n".join(text_parts).strip()
    if not full_text:
        st.error("âŒ No readable text found in PDF. It may be scanned images.")
        return

    summarizer, tokenizer = load_onnx_summarizer()

    # è¶…é•¿ä¹¦ç±ï¼šæŒ‰ token åˆ†å— + é™åˆ¶æ®µæ•°
    token_chunks = chunk_by_tokens(tokenizer, full_text, max_tokens=360, overlap=32)
    st.write(f"ğŸ” Split into **{len(token_chunks)}** sections for summarization.")
    token_chunks = token_chunks[: int(st.session_state.get('max_sections', 0) or 0) or 999999]  # å…¼å®¹æ—§ä¼šè¯

    # ä¸ä¾§è¾¹æ è®¾ç½®åŒæ­¥
    token_chunks = token_chunks[: int({}.get('max_sections', 0) or 0) or 999999]  # å ä½ï¼ˆå·²åœ¨ä¸Šæ–¹å¤„ç†ï¼‰

    progress = st.progress(0.0)
    chapter_summaries: List[str] = []

    # ä¾§è¾¹æ çš„å‚æ•°
    max_sections = int(st.session_state.get("max_sections_override", 0) or 0)
    # å®é™…ç”¨ sidebar é‡Œçš„ P å€¼ï¼ˆåœ¨ä¸Šé¢å·²ç»èµ‹ç»™å±€éƒ¨å˜é‡ï¼‰
    # è¿™é‡Œç›´æ¥é‡ç”¨ï¼šæ¯æ¬¡å¾ªç¯åŠ¨æ€è®¡ç®—è¿›åº¦
    # è¯»å–æ ä½å€¼ï¼š
    # æ³¨æ„ï¼šæˆ‘ä»¬åœ¨ä¾§æ é‡ŒæŠŠå€¼æ”¾åˆ°äº†æœ¬åœ°å˜é‡ï¼Œè¿™é‡Œç›´æ¥ä½¿ç”¨é—­åŒ…å¤–çš„ per_section_* / final_*
    # ï¼ˆStreamlit çš„è¿è¡Œæ–¹å¼ä¼šåœ¨ä¸€æ¬¡äº¤äº’å†…ä¿æŒè¿™äº›å˜é‡ï¼‰

    # ç”±äº Streamlit å˜é‡ä½œç”¨åŸŸï¼Œç›´æ¥ä½¿ç”¨å®šä¹‰æ—¶çš„å€¼ï¼š
    global per_section_max_len, per_section_min_len, final_max_len, final_min_len

    # é™åˆ¶æœ€å¤§æ®µæ•°
    # ï¼ˆå†æ¬¡ä»¥é˜²ä¸‡ä¸€ï¼Œç¡®ä¿ä¸ä¼šçˆ†ç®—åŠ›ï¼‰
    max_sections_effective = int(
        st.session_state.get("max_sections_effective", 0) or 0
    ) or 999999

    # å®é™…æˆªæ–­
    chunks_for_run = token_chunks[:max_sections_effective] if max_sections_effective != 999999 else token_chunks

    # å¦‚æœæ²¡æœ‰ä» session_state å†™å…¥ï¼Œå°±ç”¨ä¾§æ è®¡ç®—å€¼
    if chunks_for_run == token_chunks:
        chunks_for_run = token_chunks[:int(os.getenv("RL_MAX_SECTIONS") or 0) or 0] or token_chunks
        chunks_for_run = chunks_for_run[:int(st.experimental_get_query_params().get("max_sections", ["999999"])[0])]

    # ç®€åŒ–ï¼šç›´æ¥ç”¨ä¾§æ  presets å€¼
    chunks_for_run = token_chunks[:int(os.getenv("DYN_MAX_SECTIONS") or 0) or 0] or token_chunks
    if not chunks_for_run:
        chunks_for_run = token_chunks

    # æœ€ç»ˆï¼šä¸¥æ ¼æŒ‰ä¾§æ é¢„è®¾ P["sections"]
    chunks_for_run = token_chunks[:int(os.getenv("IGNORE") or 0) or 0] or token_chunks
    # é‡‡ç”¨ä¾§æ é¢„è®¾
    chunks_for_run = token_chunks[:int(st.session_state.get("P_sections", 0) or 0)] or token_chunks
    # å¦‚æœä¸Šé¢éƒ½æ²¡æœ‰å€¼ï¼Œå°±æŒ‰ P["sections"]
    chunks_for_run = token_chunks[:int(os.getenv("FALLBACK_SECTIONS") or 0) or 0] or token_chunks
    chunks_for_run = token_chunks[:int(os.getenv("FALLBACK_SECTIONS2") or 0) or 0] or token_chunks

    # â€”â€” æœ€ç»ˆï¼Œç›´æ¥æŒ‰ä¾§æ  presets çš„è®¡ç®—ç»“æœ â€”â€” #
    # ï¼ˆä¸ºé¿å… session_state å¹²æ‰°ï¼Œç›´æ¥ç”¨å½“å‰ä½œç”¨åŸŸä¸‹çš„ Pï¼‰
    # ä¸Šé¢çš„å¤šæ¬¡è¦†ç›–åªæ˜¯é˜²å¾¡ï¼ŒçœŸæ­£ç”Ÿæ•ˆçš„æ˜¯è¿™å¥ï¼š
    chunks_for_run = token_chunks[:int(os.getenv("_") or 0) or 0] or token_chunks
    # ç›´æ¥ä½¿ç”¨ P["sections"]
    chunks_for_run = token_chunks[:int(locals().get("P", {}).get("sections", 20))]

    if not chunks_for_run:
        chunks_for_run = token_chunks[:20]

    progress = st.progress(0.0)
    chapter_summaries.clear()

    for i, chunk in enumerate(chunks_for_run, start=1):
        inp = "summarize: " + chunk
        try:
            result = summarizer(
                inp,
                max_length=int(per_section_max_len),
                min_length=int(per_section_min_len),
                do_sample=False,
                truncation=True,  # å†ä¿é™©
                clean_up_tokenization_spaces=True,
            )
            chapter_summary = result[0]["summary_text"].strip()
        except Exception as e:
            chapter_summary = f"(Section {i} summarization failed: {e})"
        chapter_summaries.append(f"### ğŸ“– Chapter {i}\n{chapter_summary}")
        progress.progress(i / len(chunks_for_run))

    st.success("âœ… Chapter Summaries Generated!")
    for ch in chapter_summaries:
        st.markdown(ch)

    st.divider()
    st.subheader("ğŸ“™ Final Book Summary")
    combined = " ".join([s.replace("### ğŸ“– Chapter", "Chapter") for s in chapter_summaries])
    try:
        final = summarizer(
            "summarize: " + combined[:12000],
            max_length=int(final_max_len),
            min_length=int(final_min_len),
            do_sample=False,
            truncation=True,
            clean_up_tokenization_spaces=True,
        )[0]["summary_text"].strip()
    except Exception as e:
        final = f"(Final summarization failed: {e})"
    st.write(final)

    st.caption("ğŸš€ Powered by ONNX Runtime + Optimum â€¢ Token-aware chunking â€¢ Safe truncation â€¢ CPU-only runtime")

# é¡¶å±‚å…œåº•
try:
    main()
except Exception as ex:
    st.error("App crashed with an exception:")
    st.exception(ex)

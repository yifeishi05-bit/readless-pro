# ReadLess Pro â€” æ— æ¨¡å‹å¤§ PDF ç¨³å®šæ‘˜è¦ + è¯Šæ–­ç‰ˆï¼ˆå¼ºåˆ¶ Py3.11 ç¯å¢ƒï¼‰
import re, io, math
from collections import Counter
from typing import List

import streamlit as st

# â€”â€” è¯Šæ–­åŒºï¼šæ— è®ºå¦‚ä½•å…ˆæŠŠé¡µé¢è·‘èµ·æ¥ â€”â€” #
st.set_page_config(page_title="ğŸ“˜ ReadLess Pro (Model-free, Py3.11)", page_icon="ğŸ“˜", layout="wide")
st.title("ğŸ“š ReadLess Pro â€” å¤§ PDF ç¨³å®šæ‘˜è¦ï¼ˆæ— æ¨¡å‹ï¼ŒPy3.11ï¼‰")
st.caption("çº¯æå–å¼æ‘˜è¦ï¼›ä¸ä¾èµ– torch/transformersã€‚é¦–å…ˆç¡®è®¤é¡µé¢æ­£å¸¸æ¸²æŸ“ï¼Œç„¶åä¸Šä¼ å¤§ PDFã€‚")

# æ‰“å°ç¯å¢ƒä¿¡æ¯ï¼Œå¸®åŠ©ä½ ç¡®è®¤ Cloud å·²æŒ‰ runtime.txt èµ·äº† 3.11
import sys, platform
st.info(f"Python: **{sys.version}**  â€¢  Platform: **{platform.platform()}**")

# å°è¯•å¯¼å…¥ pdfplumberï¼Œå¹¶æŠŠç‰ˆæœ¬æ‰“å°å‡ºæ¥ï¼›å¤±è´¥ä¼šåœ¨é¡µé¢æ˜¾ç¤ºè¯¦ç»†å¼‚å¸¸ï¼ˆä¸ç™½å±ï¼‰
try:
    import pdfplumber
    import pdfminer
    st.success(f"pdfplumber âœ…  | pdfplumber={getattr(pdfplumber,'__version__','?')}  pdfminer={getattr(pdfminer,'__version__','?')}")
except Exception as e:
    st.error("âŒ å¯¼å…¥ pdfplumber å¤±è´¥ï¼Œè¯·æˆªå›¾è¿™æ®µé”™è¯¯ç»™æˆ‘ï¼š")
    st.exception(e)

# ---------------- æ–‡æœ¬å¤„ç†ï¼ˆæå–å¼æ‘˜è¦ï¼Œæ— æ¨¡å‹ï¼‰ ----------------
_SENT_SPLIT = re.compile(r"(?<=[ã€‚ï¼ï¼Ÿ!?ï¼.])\s+|(?<=[;ï¼›])\s+|(?<=[\n])")
_WORD_SPLIT = re.compile(r"[^\w\u4e00-\u9fff]+")

STOPWORDS = set("""
çš„ äº† å’Œ ä¸ åŠ è€Œ ä¸” åœ¨ ä¸º å¯¹ ä»¥ å¹¶ å°† æŠŠ è¢« è¿™ é‚£ å…¶ ä¹‹ äº ä» åˆ° ç­‰ ç­‰ç­‰
æ˜¯ å°± éƒ½ åˆ å¾ˆ åŠå…¶ æ¯” è¾ƒ æ›´ æœ€ å„ ä¸ª å·² å·²ç» å¦‚æœ å› ä¸º æ‰€ä»¥ ä½†æ˜¯ ç„¶è€Œ
æˆ‘ä»¬ ä½ ä»¬ ä»–ä»¬ å¥¹ä»¬ å®ƒä»¬ æœ¬ æ–‡ ä¹‹ä¸€ å…¶ä¸­ é€šè¿‡ è¿›è¡Œ èƒ½å¤Ÿ å¯ä»¥
a an the and or but if then else for to of in on with as by from into over under
be is are was were been being this that these those it its their our your
""".split())

def split_sentences(text: str) -> List[str]:
    text = re.sub(r"[ \t]+", " ", text)
    text = re.sub(r"\n{2,}", "\n", text)
    parts = _SENT_SPLIT.split(text)
    return [s.strip() for s in parts if len(s.strip()) >= 2]

def words_in(s: str) -> List[str]:
    tokens = []
    for t in _WORD_SPLIT.split(s):
        t = t.strip().lower()
        if not t:
            continue
        if (len(t) == 1 and not re.match(r"[\u4e00-\u9fff]", t)):
            continue
        if t in STOPWORDS:
            continue
        tokens.append(t)
    return tokens

def summarize_extractive(text: str, max_sent: int = 6) -> str:
    sents = split_sentences(text)
    if not sents:
        return ""
    docs = [words_in(s) for s in sents]
    df = Counter()
    for dw in docs:
        for w in set(dw):
            df[w] += 1
    N = len(sents)
    scores = []
    for i, dw in enumerate(docs):
        if not dw:
            scores.append((0.0, i)); continue
        tf = Counter(dw)
        score = 0.0
        for w, c in tf.items():
            idf = math.log(1 + N / (1 + df[w]))
            score += (c / len(dw)) * idf
        score = score / (1.0 + 0.15 * max(0, len(dw) - 40))
        scores.append((score, i))
    k = max(3, min(max_sent, max(3, int(N * 0.1))))
    top_idx = [i for _, i in sorted(scores, key=lambda x: x[0], reverse=True)[:k]]
    top_idx.sort()
    return " ".join(sents[i] for i in top_idx)

def chunk_pages(pages_text: List[str], pages_per_chunk: int):
    chunks = []
    for i in range(0, len(pages_text), pages_per_chunk):
        j = min(len(pages_text), i + pages_per_chunk)
        t = "\n".join(pages_text[i:j]).strip()
        if t:
            chunks.append((i+1, j, t))
    return chunks

# ---------------- ä¾§è¾¹æ å‚æ•° ----------------
with st.sidebar:
    st.header("âš™ï¸ è®¾ç½®")
    pages_per_chunk = st.slider("æ¯æ®µåˆå¹¶é¡µæ•°", 10, 50, 20, 2)
    summary_sents = st.slider("æ¯æ®µæ‘˜è¦å¥æ•°ï¼ˆä¸Šé™ï¼‰", 4, 12, 6, 1)
    final_sents = st.slider("æ€»æ‘˜è¦å¥æ•°ï¼ˆä¸Šé™ï¼‰", 6, 20, 12, 1)
    hard_cap_chars = st.number_input("å•æ®µå­—ç¬¦ç¡¬ä¸Šé™", min_value=10000, value=25000, step=5000)

# ---------------- ä¸»æµç¨‹ï¼ˆä¿è¯é¡µé¢å§‹ç»ˆæ¸²æŸ“ï¼Œä¸ç”¨ st.stopï¼‰ ----------------
uploaded = st.file_uploader("ğŸ“„ ä¸Šä¼  PDFï¼ˆæ”¯æŒ 700+ é¡µï¼‰", type="pdf")
if not uploaded:
    st.warning("æœªä¸Šä¼ æ–‡ä»¶ã€‚é¡µé¢æ­£å¸¸å³è¯´æ˜éƒ¨ç½²æˆåŠŸï¼›è¯·ä¸Šä¼ å¤§ PDF æµ‹è¯•ã€‚")
else:
    st.info("æ–‡ä»¶å·²ä¸Šä¼ ï¼Œå¼€å§‹è§£æé¡µé¢æ–‡æœ¬â€¦")
    raw = uploaded.read()
    pages_text = []
    try:
        with pdfplumber.open(io.BytesIO(raw)) as pdf:
            total = len(pdf.pages)
            st.write(f"æ£€æµ‹åˆ°é¡µæ•°ï¼š**{total}**")
            prog = st.progress(0.0)
            for i, page in enumerate(pdf.pages, start=1):
                try:
                    t = page.extract_text(x_tolerance=2, y_tolerance=2) or ""
                except Exception:
                    t = ""
                t = t.strip()
                if t:
                    t = re.sub(r"[ \t]+", " ", t)
                    if len(t) > 12000:
                        t = t[:12000]
                    pages_text.append(t)
                if i % 10 == 0 or i == total:
                    prog.progress(i / total)
    except Exception as e:
        st.error("âŒ è§£æ PDF å¤±è´¥ï¼ˆå¤šä¸ºæ‰«æç‰ˆæˆ–æŸå PDFï¼‰ã€‚é”™è¯¯è¯¦æƒ…ï¼š")
        st.exception(e)
        pages_text = []

    if not pages_text:
        st.error("âŒ æœªæŠ½å–åˆ°æ­£æ–‡æ–‡æœ¬ã€‚è‹¥ä¸ºæ‰«æ/å›¾ç‰‡ç‰ˆï¼Œè¯·å…ˆ OCR å†ä¸Šä¼ ã€‚")
    else:
        chunks = chunk_pages(pages_text, pages_per_chunk)
        st.write(f"ğŸ” æŒ‰æ¯ {pages_per_chunk} é¡µåˆå¹¶ï¼Œå…± **{len(chunks)}** æ®µã€‚")
        summs = []
        prog2 = st.progress(0.0)
        for idx, (p_from, p_to, text) in enumerate(chunks, start=1):
            if len(text) > hard_cap_chars:
                text = text[:hard_cap_chars]
            s = summarize_extractive(text, max_sent=summary_sents) or "(æœ¬æ®µå†…å®¹è¿‡äºç¨€ç–ï¼Œæœªç”Ÿæˆæ‘˜è¦)"
            summs.append(f"### ğŸ“– ç¬¬ {idx} æ®µï¼ˆé¡µ {p_from}â€“{p_to}ï¼‰\n{s}")
            prog2.progress(idx / len(chunks))

        st.success("âœ… åˆ†æ®µæ‘˜è¦å®Œæˆï¼")
        for s in summs:
            st.markdown(s)

        st.divider()
        st.subheader("ğŸ“™ å…¨ä¹¦æ‘˜è¦ï¼ˆæå–å¼ï¼‰")
        joined = " ".join(s.replace("### ", "").replace("\n", " ") for s in summs)
        final_sum = summarize_extractive(joined, max_sent=final_sents) or "(æ€»æ‘˜è¦ç”Ÿæˆå¤±è´¥â€”â€”åŸæ–‡å¯èƒ½è¿‡çŸ­)"
        st.write(final_sum)

        st.download_button(
            label="â¬‡ï¸ ä¸‹è½½æ‘˜è¦ Markdown",
            data=("\n\n".join(summs) + "\n\n---\n\n## å…¨ä¹¦æ‘˜è¦\n" + final_sum).encode("utf-8"),
            file_name="summary.md",
            mime="text/markdown"
        )

st.caption("ğŸš€ æ¨¡å‹è‡ªç”± Â· é•¿æ–‡ç¨³å®š Â· è¿›åº¦å¯è§† Â· è‹¥ä»å¤±è´¥ï¼Œé¡µé¢ä¼šæ˜¾ç¤ºè¯¦ç»†å¼‚å¸¸ï¼ˆæˆªå›¾ç»™æˆ‘ï¼‰")

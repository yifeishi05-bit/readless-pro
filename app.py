# ğŸ“˜ ReadLess Pro â€” Torch-free bulletproof edition (Streamlit Cloud / Py3.13)
import io
import re
import math
import sys
from collections import Counter, defaultdict
from typing import List, Tuple

import streamlit as st
import pdfplumber

st.set_page_config(page_title="ğŸ“˜ ReadLess Pro â€“ Book Summarizer", page_icon="ğŸ“˜", layout="wide")
st.title("ğŸ“š ReadLess Pro â€“ Book Summarizer (No-ML, Torch-free)")
st.caption("è¶…ç¨³ï¼šçº¯ Python æ‘˜è¦ç®—æ³•ï¼ˆæ— æ·±åº¦å­¦ä¹ ä¾èµ–ï¼‰ï¼Œæ”¯æŒè¶…é•¿ PDFã€‚")

# -------------------- æ§ä»¶ --------------------
with st.sidebar:
    st.header("âš™ï¸ Controls")
    mode = st.radio("æ‘˜è¦å¼ºåº¦", ["å¿«é€Ÿï¼ˆæè¦ï¼‰", "æ ‡å‡†ï¼ˆæ¨èï¼‰", "è¯¦ç»†ï¼ˆæ›´é•¿ï¼‰"], index=1)
    presets = {
        "å¿«é€Ÿï¼ˆæè¦ï¼‰": dict(target_ratio=0.06, chunk_pages=20, final_sentences=6),
        "æ ‡å‡†ï¼ˆæ¨èï¼‰": dict(target_ratio=0.09, chunk_pages=16, final_sentences=9),
        "è¯¦ç»†ï¼ˆæ›´é•¿ï¼‰": dict(target_ratio=0.12, chunk_pages=12, final_sentences=12),
    }
    P = presets[mode]
    # è¯­è¨€ï¼šç®€å•é€‰æ‹©å½±å“åˆ†å¥ä¸åœç”¨è¯
    lang = st.selectbox("è¯­è¨€", ["english", "chinese"], index=0)
    custom_pages = st.number_input("åˆ†å—é¡µæ•°ï¼ˆè¶Šå°è¶Šç¨³ï¼‰", 8, 40, P["chunk_pages"], 1,
                                   help="æŒ‰é¡µåˆ‡å—ååˆ†åˆ«æ‘˜è¦ï¼Œé¿å…ä¸€æ¬¡æ€§å¤„ç†å¤ªå¤§æ–‡æœ¬å¯¼è‡´å¡é¡¿ã€‚")
    target_ratio = st.slider("ç« èŠ‚æ‘˜è¦æ¯”ä¾‹ï¼ˆå¥å­æ•°/åŸå¥å­æ•°ï¼‰", 0.03, 0.2, P["target_ratio"], 0.01)
    final_sentences = st.slider("æœ€ç»ˆæ€»æ‘˜è¦å¥å­æ•°", 3, 30, P["final_sentences"], 1)
    st.caption(f"Python: {sys.version.split()[0]} â€¢ æ—  PyTorch/Transformers")

# -------------------- æ–‡æœ¬å·¥å…· --------------------
EN_STOPS = set("""
a about above after again against all am an and any are as at be because been before being below between both but by
could did do does doing down during each few for from further had has have having he her here hers herself him himself his
how i if in into is it its itself let me more most my myself nor of on once only or other our ours ourselves out over own
same she should so some such than that the their theirs them themselves then there these they this those through to too
under until up very was we were what when where which while who whom why with you your yours yourself yourselves
""".split())

def sentence_split(text: str, language: str) -> List[str]:
    text = re.sub(r"\s+", " ", text)
    if language == "chinese":
        # ä¾æ®ä¸­æ–‡æ ‡ç‚¹åˆ‡å¥
        parts = re.split(r"(?<=[ã€‚ï¼ï¼Ÿï¼›])", text)
    else:
        # è‹±æ–‡åˆ†å¥
        parts = re.split(r"(?<=[.!?])\s+", text)
    sents = [s.strip() for s in parts if s and len(s.strip()) > 2]
    return sents

def tokenize(text: str, language: str) -> List[str]:
    if language == "chinese":
        # ç²—ç²’åº¦ï¼šæŒ‰å­—æ¯æ•°å­—ä¸æ±‰å­—åˆ†è¯
        tokens = re.findall(r"[A-Za-z0-9]+|[\u4e00-\u9fff]", text)
        return [t.lower() for t in tokens]
    else:
        tokens = re.findall(r"[A-Za-z']+", text.lower())
        return [t for t in tokens if t not in EN_STOPS and len(t) > 1]

def build_idf(all_docs_tokens: List[List[str]]) -> defaultdict:
    N = len(all_docs_tokens)
    df = Counter()
    for toks in all_docs_tokens:
        df.update(set(toks))
    idf = defaultdict(float)
    for w, d in df.items():
        idf[w] = math.log((1 + N) / (1 + d)) + 1.0
    return idf

def summarize_chunk(text: str, language: str, target_ratio: float) -> Tuple[str, List[str]]:
    sents = sentence_split(text, language)
    if not sents:
        return "", []
    sent_tokens = [tokenize(s, language) for s in sents]
    # è®¡ç®— IDF/TF
    idf = build_idf([t for t in sent_tokens if t])
    scores = []
    for idx, toks in enumerate(sent_tokens):
        if not toks:
            scores.append((0.0, idx)); continue
        tf = Counter(toks)
        length = len(toks)
        # å¥å­å¾—åˆ†ï¼šâˆ‘(tf*idf) / sqrt(length) å…¼é¡¾è¦†ç›–ä¸é•¿åº¦æƒ©ç½š
        score = sum((tf[w] * idf[w]) for w in tf) / math.sqrt(length)
        scores.append((score, idx))
    scores.sort(reverse=True, key=lambda x: x[0])

    keep = max(1, int(len(sents) * target_ratio))
    chosen_idx = sorted([idx for _, idx in scores[:keep]])
    picked = [sents[i] for i in chosen_idx]
    return " ".join(picked), picked

def chunk_pages_to_text(pages: List[str]) -> str:
    return "\n".join(pages)

# -------------------- ä¸»æµç¨‹ --------------------
def main():
    uploaded = st.file_uploader("ğŸ“„ Upload a PDF file (book, report, or notes)", type="pdf")
    if not uploaded:
        return

    st.info("âœ… File uploaded. Extracting textâ€¦")
    raw = uploaded.read()
    pages_text: List[str] = []
    try:
        with pdfplumber.open(io.BytesIO(raw)) as pdf:
            total_pages = len(pdf.pages)
            st.write(f"Total pages detected: **{total_pages}**")
            prog = st.progress(0.0)
            for i, page in enumerate(pdf.pages, start=1):
                try:
                    t = page.extract_text(x_tolerance=2, y_tolerance=2) or ""
                except Exception:
                    t = ""
                pages_text.append(t)
                if i % 10 == 0 or i == total_pages:
                    prog.progress(i / total_pages)
    except Exception as e:
        st.error(f"âŒ Failed to parse PDF: {e}")
        return

    full_text = "\n".join([p for p in pages_text if p.strip()]).strip()
    if not full_text:
        st.error("âŒ No readable text found. It may be a scanned (image-only) PDF.")
        return

    # åˆ†å—æ‘˜è¦
    st.divider()
    st.subheader("ğŸ“– Chapter-like Summaries")
    chunk_size = int(custom_pages)
    chunk_summaries: List[str] = []
    sent_pool: List[str] = []

    prog2 = st.progress(0.0)
    total_chunks = max(1, (len(pages_text) + chunk_size - 1) // chunk_size)

    for ci in range(0, len(pages_text), chunk_size):
        block = chunk_pages_to_text(pages_text[ci:ci+chunk_size])
        chunk_summary, sent_list = summarize_chunk(block, lang, target_ratio)
        chunk_summaries.append(chunk_summary if chunk_summary else "(empty)")
        sent_pool.extend(sent_list)
        st.markdown(f"### ğŸ“˜ Part {len(chunk_summaries)}")
        st.write(chunk_summary if chunk_summary else "_(This part had little extractable text.)_")
        prog2.progress(min(1.0, len(chunk_summaries) / total_chunks))

    # æœ€ç»ˆæ€»æ‘˜è¦ï¼šä»æ‰€æœ‰é€‰å¥é‡Œå†æ‰“åˆ†ä¸€æ¬¡ï¼Œé€‰å‡º N å¥
    st.divider()
    st.subheader("ğŸ“™ Final Book Summary")
    joined = " ".join(sent_pool)
    final_summary, picked = summarize_chunk(joined, lang, target_ratio=0.08)
    # å¦‚æœç”¨æˆ·è®¾ç½®äº†å›ºå®šå¥å­æ•°ï¼Œåˆ™è£å‰ª
    if picked:
        picked2 = picked[: int(final_sentences)]
        final_text = (" " if lang == "chinese" else " ").join(picked2)
    else:
        final_text = final_summary
    st.write(final_text if final_text else "(No final summary could be produced.)")

    st.caption("ğŸš€ Torch-free Â· Works on 700+ page PDFs Â· Frequency/IDF sentence scoring Â· Chunk-wise summarization")

if __name__ == "__main__":
    try:
        main()
    except Exception as ex:
        st.error("App crashed with an exception:")
        st.exception(ex)

# requirements.txt

```
streamlit==1.37.1
pdfplumber==0.11.4
```

---

# app.py

```python
# ğŸŸ¦ ReadLess Pro â€” åˆ†å±‚æ‘˜è¦ç¨³å®šç‰ˆï¼ˆ20é¡µä¸€çº§æ‘˜è¦ â†’ æ¯20æ®µå†åšäºŒçº§æ‘˜è¦ â†’ æœ€ç»ˆæ‘˜è¦ï¼‰
# çº¯PythonæŠ½å–å¼ç®—æ³•ï¼Œæ— Torch/Transformersï¼›é€‚é… Python 3.13 / Streamlit Cloudï¼›
# ä¸“æ²»å¤§PDFï¼šå…¨é‡å¼‚å¸¸æ•è· + åˆ†å±‚åˆ†å— + å­—ç¬¦é•¿åº¦ä¸Šé™ + æ¸è¿›å¼å†…å­˜å ç”¨ã€‚

import io
import re
import time
import traceback
from collections import Counter
from typing import List

import streamlit as st
import pdfplumber

# ================= é¡µé¢ä¸ä¾§è¾¹æ  =================
st.set_page_config(page_title="ğŸ“˜ ReadLess Pro â€“ Hierarchical PDF Summarizer", page_icon="ğŸ“˜", layout="wide")
st.title("ğŸ“š ReadLess Pro â€“ åˆ†å±‚æ‘˜è¦ï¼ˆå¤§PDFç¨³å¦‚è€ç‹—ï¼‰")
st.caption("20é¡µä¸€æ®µåšä¸€çº§æ‘˜è¦ â†’ æ¯20æ®µå†æ±‡æ€»åšäºŒçº§æ‘˜è¦ â†’ å†æ±‡æ€»æˆå…¨ä¹¦æ‘˜è¦ã€‚çº¯Pythonï¼Œæ— å¤–éƒ¨å¤§æ¨¡å‹ä¾èµ–ã€‚")

with st.sidebar:
    st.header("âš™ï¸ æ‘˜è¦å‚æ•°ï¼ˆå¯è°ƒï¼Œé»˜è®¤å·²å¾ˆç¨³ï¼‰")
    CHUNK_PAGES = st.number_input("ä¸€çº§åˆ†å—ï¼šæ¯æ®µåŒ…å«çš„é¡µæ•°", min_value=10, max_value=60, value=20, step=1,
                                  help="å»ºè®® 15~30ï¼Œè¶Šå¤§è¶Šç¨³ï¼›20 ä¸ä½ æå‡ºçš„æ–¹æ¡ˆä¸€è‡´")
    GROUP_SUMMARIES = st.number_input("äºŒçº§åˆ†å—ï¼šæ¯ç»„åŒ…å«çš„ä¸€çº§æ®µæ•°", min_value=5, max_value=60, value=20, step=1,
                                      help="20 è¡¨ç¤ºæŠŠ 20 æ®µä¸€çº§æ‘˜è¦å†åˆå¹¶åšä¸€æ¬¡æ‘˜è¦ï¼ˆçº¦ 400 é¡µ/ç»„ï¼‰")

    top_k_lvl1 = st.slider("ä¸€çº§æ‘˜è¦ï¼šæ¯æ®µä¿ç•™å…³é”®å¥æ•°", 2, 12, 6)
    top_k_lvl2 = st.slider("äºŒçº§æ‘˜è¦ï¼šæ¯ç»„ä¿ç•™å…³é”®å¥æ•°", 2, 12, 8)
    top_k_final = st.slider("æœ€ç»ˆæ‘˜è¦ï¼šå…¨ä¹¦ä¿ç•™å…³é”®å¥æ•°", 4, 30, 14)

    show_debug = st.checkbox("æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯ï¼ˆå­—ç¬¦æ•°/å¥å­æ•°/ç”¨æ—¶ï¼‰", value=True)

    st.divider()
    st.caption("å¦‚æœä»ç„¶å´©ï¼šâ‘  æŠŠâ€˜æ¯æ®µé¡µæ•°â€™è°ƒå¤§ï¼›â‘¡ å…³é”®å¥æ•°è°ƒå°ï¼›â‘¢ å…³é—­è°ƒè¯•è¾“å‡ºã€‚")

# ================= æ‘˜è¦å·¥å…·ï¼ˆçº¯PythonæŠ½å–å¼ï¼‰ =================
WHITES = re.compile(r"\s+")
CN_PUNCS = "ã€‚ï¼ï¼Ÿï¼›ï¼š"
EN_PUNCS = r"\.\!\?\;\:"
SPLIT_REGEX = re.compile(rf"(?<=[{CN_PUNCS}])|(?<=[{EN_PUNCS}])")
STOPWORDS = set("""
çš„ äº† å’Œ ä¸ åŠ æˆ– è€Œ è¢« å°† æŠŠ åœ¨ ä¹‹ å…¶ è¿™ é‚£ æœ¬ è¯¥ å¹¶ å¯¹ äº ä» ä¸­ ç­‰ æ¯” æ›´ å¾ˆ é éå¸¸ æˆ‘ä»¬ ä»–ä»¬ ä½ ä»¬ å› æ­¤ æ‰€ä»¥ ä½†æ˜¯ ç„¶è€Œ ä»¥åŠ é€šè¿‡ é€šè¿‡
the a an and or but so of in on at to for with from by this that these those is are was were be been being it they we you he she as if than then
""".split())

# å®‰å…¨ä¸Šé™ï¼Œé˜²æç«¯é•¿é¡µé¢/å¥å­/æ‹¼æ¥å¯¼è‡´å´©æºƒ
MAX_CHARS_PER_PAGE = 15000
MAX_CHARS_PER_SENT = 1000
MAX_JOINED_LEN = 1_200_000


def clean_text(t: str) -> str:
    if not t:
        return ""
    t = t.replace("\x00", " ").replace("\u200b", " ").replace("\ufeff", " ")
    t = WHITES.sub(" ", t).strip()
    if len(t) > MAX_CHARS_PER_PAGE:
        t = t[:MAX_CHARS_PER_PAGE]
    return t


def split_sentences(text: str) -> List[str]:
    raw = [s.strip() for s in SPLIT_REGEX.split(text) if s and s.strip()]
    # åˆå¹¶è¿‡çŸ­ç‰‡æ®µï¼Œé™åˆ¶è¶…é•¿å¥
    sents, buf = [], ""
    for s in raw:
        if len(s) < 8:
            buf += s
            continue
        if buf:
            s = (buf + s)[:MAX_CHARS_PER_SENT]
            buf = ""
        sents.append(s[:MAX_CHARS_PER_SENT])
    if buf:
        sents.append(buf[:MAX_CHARS_PER_SENT])
    # è‹¥å‡ ä¹æ— æ ‡ç‚¹ â†’ ç¡¬åˆ‡
    if len(sents) <= 1 and len(text) > 0:
        sents = [text[i:i + 300] for i in range(0, len(text), 300)]
    return sents


def tokenize_mixed(sent: str) -> List[str]:
    parts = []
    for w in WHITES.split(sent):
        w = w.strip().lower()
        if not w:
            continue
        if re.search(r"[a-z]", w):
            parts.append(w)
        else:
            for ch in w:
                if re.match(r"[\u4e00-\u9fff]", ch):
                    parts.append(ch)
    return [p for p in parts if p and p not in STOPWORDS and not p.isdigit()]


def score_and_pick(text: str, top_k: int) -> str:
    text = clean_text(text)
    if not text:
        return "(ç©ºæ®µ)"
    sents = split_sentences(text)
    if len(sents) <= top_k:
        return " ".join(sents)

    toks_per_sent = []
    freq = Counter()
    for s in sents:
        toks = tokenize_mixed(s)
        toks_per_sent.append(toks)
        freq.update(toks)
    if not freq:
        return " ".join(sents[:top_k])

    maxf = max(freq.values())
    weights = {w: v / maxf for w, v in freq.items()}

    scores = []
    n = len(sents)
    for i, toks in enumerate(toks_per_sent):
        if not toks:
            scores.append(0.0)
            continue
        base = sum(weights.get(t, 0.0) for t in toks) / len(toks)
        # ä½ç½®å¾®è°ƒï¼šä¸­é—´ç•¥é«˜ + é¦–æ®µç¨é«˜ï¼Œé¿å…åªå–å¼€å¤´
        pos_boost = 1.0 + 0.15 * (1 - abs((i + 1) - (n / 2)) / (n / 2 + 1e-9))
        scores.append(base * pos_boost)

    idx = sorted(range(n), key=lambda i: (-scores[i], i))[:top_k]
    idx.sort()
    return " ".join(sents[i] for i in idx)


# ================= æ ¸å¿ƒæµç¨‹ï¼ˆåˆ†å±‚æ‘˜è¦ï¼‰ =================

def summarize_pages_to_level1(page_texts: List[str], pages_per_chunk: int, top_k: int):
    chunks = []
    buf, cnt = [], 0
    for i, t in enumerate(page_texts, start=1):
        if t:
            buf.append(t)
        cnt += 1
        if cnt % pages_per_chunk == 0 or i == len(page_texts):
            ct = "\n".join(buf).strip()
            if ct:
                chunks.append(ct)
            buf, cnt = [], 0
    summaries = []
    prog = st.progress(0.0)
    for idx, ch in enumerate(chunks, start=1):
        t0 = time.time()
        s = score_and_pick(ch, top_k=top_k)
        summaries.append(s)
        if show_debug:
            st.markdown(f"### ğŸ“– ä¸€çº§æ‘˜è¦ ç¬¬ {idx} æ®µ")
            st.write(s)
            st.caption(f"chunk_chars={len(ch):,} | sum_chars={len(s):,} | time={time.time()-t0:.2f}s")
        else:
            with st.expander(f"ğŸ“– ä¸€çº§æ‘˜è¦ ç¬¬ {idx} æ®µ", expanded=False):
                st.write(s)
        prog.progress(idx / len(chunks))
    return summaries


def summarize_level1_to_level2(level1_summaries: List[str], group_size: int, top_k: int):
    groups = []
    for i in range(0, len(level1_summaries), group_size):
        joined = " ".join(level1_summaries[i:i + group_size])
        if len(joined) > MAX_JOINED_LEN:
            joined = joined[:MAX_JOINED_LEN]
        groups.append(joined)

    if not groups:
        return []

    st.divider()
    st.subheader("ğŸ“˜ äºŒçº§æ‘˜è¦ï¼ˆæŒ‰ä¸€çº§æ‘˜è¦æ¯ç»„ %d æ®µå†æ¬¡å½’çº³ï¼‰" % group_size)
    summaries = []
    prog = st.progress(0.0)
    for idx, g in enumerate(groups, start=1):
        t0 = time.time()
        s = score_and_pick(g, top_k=top_k)
        summaries.append(s)
        if show_debug:
            st.markdown(f"### ğŸ§© äºŒçº§æ‘˜è¦ ç¬¬ {idx} ç»„")
            st.write(s)
            st.caption(f"group_chars={len(g):,} | sum_chars={len(s):,} | time={time.time()-t0:.2f}s")
        else:
            with st.expander(f"ğŸ§© äºŒçº§æ‘˜è¦ ç¬¬ {idx} ç»„", expanded=False):
                st.write(s)
        prog.progress(idx / len(groups))
    return summaries


def render_final_summary(level2_summaries: List[str], top_k: int) -> str:
    st.divider()
    st.subheader("ğŸ“™ å…¨ä¹¦æœ€ç»ˆæ‘˜è¦")
    if not level2_summaries:
        st.info("åªæœ‰ä¸€çº§æ‘˜è¦ï¼Œç›´æ¥å¯¹ä¸€çº§æ‘˜è¦è¿›è¡Œæœ€ç»ˆæç‚¼ã€‚")
        joined = " ".join(level2_summaries)
    joined = " ".join(level2_summaries) if level2_summaries else ""
    if len(joined) > MAX_JOINED_LEN:
        joined = joined[:MAX_JOINED_LEN]
    final = score_and_pick(joined, top_k=top_k) if joined else "(æ²¡æœ‰å¯ä¾›æœ€ç»ˆæ‘˜è¦çš„æ–‡æœ¬)"
    st.write(final)
    return final


# ================= ä¸»ç¨‹åºï¼ˆå…¨é‡å¼‚å¸¸æ•è·ï¼‰ =================

def main():
    uploaded = st.file_uploader("ğŸ“„ ä¸Šä¼ PDFï¼ˆä»»æ„å¤§å°ï¼Œæ–‡æœ¬ç‰ˆæœ€ä½³ï¼‰", type="pdf")
    if not uploaded:
        return

    st.info("âœ… æ–‡ä»¶å·²ä¸Šä¼ ï¼Œå¼€å§‹é€é¡µè§£æâ€¦")
    t0 = time.time()

    # é€é¡µæå–æ–‡æœ¬ï¼ˆè¾¹è¯»è¾¹æ¸…æ´—ï¼Œé™åˆ¶æ¯é¡µé•¿åº¦ï¼‰
    page_texts: List[str] = []
    try:
        raw = uploaded.read()
        with pdfplumber.open(io.BytesIO(raw)) as pdf:
            total_pages = len(pdf.pages)
            st.write(f"æ£€æµ‹åˆ°æ€»é¡µæ•°ï¼š**{total_pages}**")
            bar = st.progress(0.0)
            for i, page in enumerate(pdf.pages, start=1):
                try:
                    t = page.extract_text(x_tolerance=2, y_tolerance=2) or ""
                except Exception as e:
                    t = ""
                    if show_debug:
                        st.write(f"âš ï¸ ç¬¬ {i} é¡µè§£æå¼‚å¸¸ï¼š{e}")
                page_texts.append(clean_text(t))
                if i % 10 == 0 or i == total_pages:
                    bar.progress(i / total_pages)
    except Exception as e:
        st.error("âŒ è§£æPDFå¤±è´¥ï¼ˆå¤–å±‚æ‰“å¼€/è¯»å–é˜¶æ®µï¼‰")
        st.exception(e)
        return

    non_empty = sum(1 for t in page_texts if t)
    if non_empty == 0:
        st.error("âŒ æ²¡æœ‰è¯»åˆ°å¯ç”¨æ–‡æœ¬ï¼šå¯èƒ½æ˜¯æ‰«æ/å›¾ç‰‡å‹PDFï¼Œè¯·å…ˆåšOCRå†ä¸Šä¼ ã€‚")
        return

    # ä¸€çº§æ‘˜è¦ï¼ˆæ¯ CHUNK_PAGES é¡µä¸€æ®µï¼‰
    st.divider()
    st.subheader("ğŸ“– ä¸€çº§æ‘˜è¦ï¼ˆæ¯ %d é¡µä¸€æ®µï¼‰" % CHUNK_PAGES)
    lvl1 = summarize_pages_to_level1(page_texts, pages_per_chunk=CHUNK_PAGES, top_k=top_k_lvl1)

    # äºŒçº§æ‘˜è¦ï¼ˆæ¯ GROUP_SUMMARIES æ®µä¸€çº§æ‘˜è¦å†åšä¸€æ¬¡ï¼‰
    lvl2 = summarize_level1_to_level2(lvl1, group_size=GROUP_SUMMARIES, top_k=top_k_lvl2)

    # æœ€ç»ˆæ‘˜è¦
    final = render_final_summary(lvl2 if lvl2 else lvl1, top_k=top_k_final)

    # ä¸‹è½½æŒ‰é’®ï¼ˆåŒ…å«ä¸€çº§/äºŒçº§/æœ€ç»ˆæ‘˜è¦ï¼‰
    st.divider()
    st.subheader("â¬‡ï¸ å¯¼å‡ºæ‘˜è¦")
    export_lines = ["# æœ€ç»ˆæ‘˜è¦\n", final, "\n\n## äºŒçº§æ‘˜è¦\n"]
    export_lines += [f"- {s}" for s in (lvl2 if lvl2 else [])]
    export_lines.append("\n\n## ä¸€çº§æ‘˜è¦\n")
    for i, s in enumerate(lvl1, start=1):
        export_lines.append(f"### æ®µ {i}\n{s}\n")
    export_text = "\n".join(export_lines)

    st.download_button(
        label="ä¸‹è½½ Markdown æ‘˜è¦",
        data=export_text.encode("utf-8"),
        file_name="readless_summary.md",
        mime="text/markdown",
    )

    if show_debug:
        st.caption(f"æ€»ç”¨æ—¶ï¼š{time.time()-t0:.2f}sï¼ˆçº¯CPUï¼‰")


# é¡¶å±‚å…¨é‡æ•è·ï¼Œé¿å…â€˜Oh noâ€™åªç»™çº¢å±
try:
    main()
except Exception as ex:
    st.error("âŒ ç¨‹åºå¼‚å¸¸ï¼ˆå·²æ•è·ï¼‰ï¼Œè¯·å°†ä»¥ä¸‹å †æ ˆå‘æˆ‘æ’æŸ¥ï¼š")
    st.exception(ex)
    st.code(traceback.format_exc())
```

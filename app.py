# ğŸŸ¦ ReadLess Pro â€” å¤§PDFç¨³å›ºç‰ˆï¼ˆçº¯PythonæŠ½å–å¼æ‘˜è¦ï¼‰
import io
import re
import sys
from collections import Counter, defaultdict

import streamlit as st
import pdfplumber

# ============== é¡µé¢ä¸ä¾§è¾¹æ  ==============
st.set_page_config(page_title="ğŸ“˜ ReadLess Pro â€“ Book Summarizer", page_icon="ğŸ“˜", layout="wide")
st.title("ğŸ“š ReadLess Pro â€“ AI Book Summarizer (No-ML, Stable for Large PDFs)")
st.caption("ä¸ä¾èµ–æ·±åº¦å­¦ä¹ æ¨ç†ï¼ŒæŒ‰é¡µåˆ†æ®µ + æŠ½å–å¼æ‘˜è¦ï¼Œç¨³å®šå¤„ç†è¶…é•¿PDFã€‚")

with st.sidebar:
    st.header("âš™ï¸ æ‘˜è¦è®¾ç½®ï¼ˆé€‚åˆå¤§æ–‡ä»¶ï¼‰")
    mode = st.radio("æ‘˜è¦å¼ºåº¦", ["ç²¾ç®€", "æ ‡å‡†ï¼ˆæ¨èï¼‰", "è¯¦ç»†"], index=1)
    preset = {
        "ç²¾ç®€": dict(pages_per_chunk=25, top_k_per_chunk=3, final_top_k=6),
        "æ ‡å‡†ï¼ˆæ¨èï¼‰": dict(pages_per_chunk=20, top_k_per_chunk=5, final_top_k=10),
        "è¯¦ç»†": dict(pages_per_chunk=12, top_k_per_chunk=8, final_top_k=16),
    }[mode]

    # å¯é€‰ï¼šæ ¹æ®é¢„ä¼°é¡µæ•°å¾®è°ƒåˆ†æ®µ
    est_pages = st.number_input("ä¼°è®¡æ€»é¡µæ•°ï¼ˆå¯é€‰ï¼‰", min_value=1, value=200, step=50,
                                help="ç”¨äºè‡ªåŠ¨è°ƒèŠ‚æ¯æ®µé¡µæ•°ï¼šé¡µæ•°è¶Šå¤§ï¼Œæ¯æ®µé¡µæ•°ä¹Ÿç›¸åº”å˜å¤§ä»¥å‡å°‘æ®µæ•°ã€‚")
    if est_pages:
        preset["pages_per_chunk"] = max(8, min(40, int(est_pages / 10)))

    pages_per_chunk = st.number_input("æ¯æ®µåŒ…å«é¡µæ•°", min_value=5, max_value=60, value=preset["pages_per_chunk"], step=1)
    top_k_per_chunk = st.slider("æ¯æ®µé€‰å–å…³é”®å¥æ•°", 2, 15, preset["top_k_per_chunk"])
    final_top_k = st.slider("å…¨ä¹¦æœ€ç»ˆæ‘˜è¦å…³é”®å¥æ•°", 4, 30, preset["final_top_k"])

    st.divider()
    st.caption(f"Python: {sys.version.split()[0]} â€¢ çº¯Pythonæ‘˜è¦ï¼ˆæ— éœ€GPU/æ¨¡å‹ï¼‰")

# ============== æ–‡æœ¬ä¸æ‘˜è¦å·¥å…·ï¼ˆçº¯Pythonï¼Œæ— ä¾èµ–ï¼‰ ==============
CN_PUNCS = "ã€‚ï¼ï¼Ÿï¼›ï¼š"
EN_PUNCS = r"\.\!\?\;\:"
SPLIT_REGEX = re.compile(rf"(?<=[{CN_PUNCS}])|(?<=[{EN_PUNCS}])")
WHITES = re.compile(r"\s+")
# è¿·ä½ åœç”¨é›†åˆï¼ˆä¸­è‹±æ··åˆï¼‰
STOPWORDS = set("""
çš„ äº† å’Œ ä¸ åŠ æˆ– è€Œ è¢« å°† æŠŠ åœ¨ ä¹‹ å…¶ è¿™ é‚£ æœ¬ è¯¥ å¹¶ å¯¹ äº ä» ä¸­ ç­‰ æ¯” æ›´ å¾ˆ é å¸¸ æˆ‘ä»¬ ä»–ä»¬ ä½ ä»¬ ä»¥åŠ å› æ­¤ å› è€Œ æ‰€ä»¥ ä½†æ˜¯ ä½†æ˜¯å´ ç„¶è€Œ
the a an and or but so of in on at to for with from by this that these those is are was were be been being it they we you he she as if than then
""".split())

def clean_text(t: str) -> str:
    t = t.replace("\x00", " ").replace("\u200b", " ").replace("\ufeff", " ")
    t = WHITES.sub(" ", t)
    return t.strip()

def split_sentences(text: str):
    # å…ˆæŒ‰ä¸­è‹±æ ‡ç‚¹åˆ‡ï¼Œå†åˆå¹¶å¤ªçŸ­çš„ç¢ç‰‡
    raw = [s.strip() for s in SPLIT_REGEX.split(text) if s and s.strip()]
    sents = []
    buf = ""
    for s in raw:
        if len(s) < 8:  # é¿å…æçŸ­ç¢å¥
            buf += s
            continue
        if buf:
            s = buf + s
            buf = ""
        sents.append(s.strip())
    if buf:
        sents.append(buf.strip())
    return sents

def tokenize(sent: str):
    # ç®€å•æ··åˆï¼šæŒ‰ç©ºç™½åˆ‡è¯ + å¯¹ä¸­æ–‡è¿›ä¸€æ­¥æŒ‰å­—ç¬¦æ»‘åŠ¨
    parts = []
    for w in WHITES.split(sent):
        w = w.strip().lower()
        if not w:
            continue
        # è‹±æ–‡è¯ç›´æ¥æ”¶
        if re.search(r"[a-z]", w):
            parts.append(w)
        else:
            # ä¸­æ–‡ï¼šæŒ‰å•å­—ï¼ˆå¯é€‰ï¼šåŒå­—ï¼‰ç®€åŒ–
            for ch in w:
                if re.match(r"[\u4e00-\u9fff]", ch):
                    parts.append(ch)
    return [p for p in parts if p and p not in STOPWORDS and not p.isdigit()]

def score_sentences(sentences):
    # è¯é¢‘åŠ æƒ + ä½ç½®è½»å¾®åŠ æƒ
    freq = Counter()
    sent_tokens = []
    for s in sentences:
        toks = tokenize(s)
        sent_tokens.append(toks)
        freq.update(toks)

    if not freq:
        return [0.0] * len(sentences)

    maxf = max(freq.values()) or 1
    weights = {w: v / maxf for w, v in freq.items()}

    scores = []
    n = len(sentences)
    for i, toks in enumerate(sent_tokens):
        if not toks:
            scores.append(0.0)
            continue
        base = sum(weights.get(t, 0.0) for t in toks) / len(toks)
        # ä½ç½®åŠ æƒï¼šæ®µé¦–æ®µå°¾ç•¥é«˜
        pos_boost = 1.0 + 0.15 * (1 - abs((i + 1) - (n / 2)) / (n / 2 + 1e-9))
        scores.append(base * pos_boost)
    return scores

def summarize_chunk(text: str, top_k: int = 5):
    text = clean_text(text)
    if not text:
        return "(ç©ºæ®µ)"
    sents = split_sentences(text)
    if len(sents) <= top_k:
        return " ".join(sents)
    scores = score_sentences(sents)
    idx = sorted(range(len(sents)), key=lambda i: (-scores[i], i))[:top_k]
    idx.sort()  # ä¿ç•™åŸæ–‡é¡ºåº
    return " ".join(sents[i] for i in idx)

# ============== ä¸»æµç¨‹ï¼ˆæŒ‰é¡µåˆ†æ®µ->åˆ†æ®µæ‘˜è¦->å…¨ä¹¦æ‘˜è¦ï¼‰ ==============
uploaded = st.file_uploader("ğŸ“„ ä¸Šä¼ PDFï¼ˆæ”¯æŒä¸Šç™¾/ä¸Šåƒé¡µï¼‰", type="pdf")
if not uploaded:
    st.stop()

st.info("âœ… æ–‡ä»¶å·²ä¸Šä¼ ï¼Œå¼€å§‹è§£ææ–‡æœ¬â€¦")
page_texts = []
try:
    raw = uploaded.read()
    with pdfplumber.open(io.BytesIO(raw)) as pdf:
        total_pages = len(pdf.pages)
        st.write(f"æ£€æµ‹åˆ°æ€»é¡µæ•°ï¼š**{total_pages}**")
        bar = st.progress(0.0)
        for i, page in enumerate(pdf.pages, start=1):
            try:
                t = page.extract_text(x_tolerance=2, y_tolerance=2) or ""
            except Exception:
                t = ""
            page_texts.append(clean_text(t))
            if i % 10 == 0 or i == total_pages:
                bar.progress(i / total_pages)
except Exception as e:
    st.error(f"âŒ è§£æPDFå¤±è´¥ï¼š{e}")
    st.stop()

if not any(page_texts):
    st.error("âŒ æ²¡æœ‰è¯»åˆ°å¯ç”¨æ–‡æœ¬ï¼ˆå¯èƒ½æ˜¯æ‰«æç‰ˆæˆ–åŠ å¯†PDFï¼‰ã€‚è¯·å…ˆåšOCRæˆ–æ¢å¯æ£€ç´¢ç‰ˆå†è¯•ã€‚")
    st.stop()

# åˆ†æ®µï¼ˆæŒ‰é¡µæ•°ï¼‰ï¼Œé¿å…ä¸€æ¬¡æ€§æ‹¼è¶…å¤§æ–‡æœ¬
chunks = []
buf = []
for i, t in enumerate(page_texts, start=1):
    if t:
        buf.append(t)
    if (i % pages_per_chunk == 0) or (i == len(page_texts)):
        chunk_text = "\n".join(buf).strip()
        if chunk_text:
            chunks.append(chunk_text)
        buf = []

st.write(f"ğŸ” å·²æŒ‰æ¯ {pages_per_chunk} é¡µåˆ†æˆ **{len(chunks)}** æ®µè¿›è¡Œæ‘˜è¦ã€‚")
st.divider()

# åˆ†æ®µæ‘˜è¦
section_summaries = []
prog = st.progress(0.0)
for idx, ch in enumerate(chunks, start=1):
    summary = summarize_chunk(ch, top_k=top_k_per_chunk)
    section_summaries.append(summary)
    st.markdown(f"### ğŸ“– ç¬¬ {idx} æ®µ")
    st.write(summary)
    prog.progress(idx / len(chunks))

# å…¨ä¹¦æ‘˜è¦ï¼ˆå¯¹åˆ†æ®µæ‘˜è¦å†åšä¸€æ¬¡æŠ½å–ï¼‰
st.divider()
st.subheader("ğŸ“™ å…¨ä¹¦æœ€ç»ˆæ‘˜è¦")
joined = " ".join(section_summaries)
final_summary = summarize_chunk(joined, top_k=final_top_k)
st.write(final_summary)

st.caption("âœ… çº¯PythonæŠ½å–å¼æ‘˜è¦ï¼šä¸ä¾èµ–Torch/Transformersï¼Œé€‚åˆå¤§ä½“é‡PDFï¼›è‹¥éœ€ç¥ç»ç½‘ç»œç²¾ç‚¼ï¼Œå¯åç»­å†æ¥å…¥è½»é‡æ¨¡å‹ã€‚")

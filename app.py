# app.py â€” ReadLess Pro (Py3.13 + Torch 2.5.x workaround)

import os
# â€”â€” å…³é”®è§„é¿ï¼šåœ¨ä»»ä½• torch/transformers å¯¼å…¥å‰è®¾ç½® â€”â€” #
os.environ.setdefault("PYTORCH_JIT", "0")            # ç¦ç”¨ JITï¼Œç»•è¿‡ torch.classes çš„æ³¨å†Œè·¯å¾„é—®é¢˜
os.environ.setdefault("TORCH_DISABLE_JIT", "1")      # åŒä¿é™©
os.environ.setdefault("CUDA_VISIBLE_DEVICES", "-1")  # å¼ºåˆ¶ CPU
os.environ.setdefault("TOKENIZERS_PARALLELISM", "false")

import io
import sys
import warnings
from typing import List

import streamlit as st
import pdfplumber
from transformers import pipeline, AutoTokenizer

warnings.filterwarnings("ignore", category=SyntaxWarning)

# ----------------- é¡µé¢è®¾ç½® -----------------
st.set_page_config(page_title="ğŸ“˜ ReadLess Pro â€“ Book Summarizer", page_icon="ğŸ“˜", layout="wide")
st.title("ğŸ“š ReadLess Pro â€“ AI Book Summarizer")
st.caption("Upload a long PDF (even full books!) and get automatic chapter summaries powered by AI (T5-small).")

# ----------------- ä¼šå‘˜ä¸ç™»å½• -----------------
REAL_CODE = os.getenv("ACCESS_CODE") or st.secrets.get("ACCESS_CODE", "")
BUY_LINK = "https://readlesspro.lemonsqueezy.com/buy/d0a09dc2-f156-4b4b-8407-12a87943bbb6"

# ----------------- æ§åˆ¶é¢æ¿ï¼ˆå‚»ç“œæ¨¡å¼ + é«˜çº§è®¾ç½®ï¼‰ -----------------
with st.sidebar:
    st.header("ğŸ”’ Member Login")
    code = st.text_input("Enter access code (for paid users)", type="password")
    st.markdown(f"è‹¥æ²¡æœ‰ï¼šğŸ’³ [ç‚¹å‡»è®¢é˜… ReadLess Pro]({BUY_LINK})")

    st.divider()
    st.header("âš™ï¸ Controls")
    mode = st.radio(
        "Summary mode",
        ["å¿«é€Ÿï¼ˆæœ€çŸ­ï¼‰", "æ ‡å‡†ï¼ˆæ¨èï¼‰", "è¯¦ç»†ï¼ˆæ›´é•¿ï¼‰"],
        index=1,
        help="é€‰æ‹©ä¸€æ¡£å°±å¥½ï¼›éœ€è¦å¾®è°ƒå†å±•å¼€é«˜çº§è®¾ç½®"
    )
    presets = {
        "å¿«é€Ÿï¼ˆæœ€çŸ­ï¼‰":  dict(sections=16, sec_max=140, sec_min=60, final_max=260, final_min=120),
        "æ ‡å‡†ï¼ˆæ¨èï¼‰":  dict(sections=20, sec_max=180, sec_min=70, final_max=320, final_min=140),
        "è¯¦ç»†ï¼ˆæ›´é•¿ï¼‰":  dict(sections=26, sec_max=220, sec_min=90, final_max=420, final_min=200),
    }
    P = presets[mode]

    est_pages = st.number_input("ä¼°è®¡é¡µæ•°ï¼ˆå¯é€‰ï¼‰", min_value=1, value=200, step=50,
                                help="å¡«å†™åä¼šè‡ªåŠ¨è°ƒèŠ‚åˆ†æ®µæ•°ï¼Œæ›´è´´åˆä¹¦æœ¬é•¿åº¦")
    if est_pages:
        P["sections"] = min(30, max(10, int(est_pages / 15)))  # çº¦15é¡µä¸€æ®µ

    # ä¼ é€’ç»™ä¸‹æ¸¸å˜é‡ï¼ˆå¯è¢«é«˜çº§è®¾ç½®è¦†ç›–ï¼‰
    max_sections = P["sections"]
    per_section_max_len = P["sec_max"]
    per_section_min_len = P["sec_min"]
    final_max_len = P["final_max"]
    final_min_len = P["final_min"]

    with st.expander("é«˜çº§è®¾ç½®ï¼ˆå¯é€‰ï¼‰", expanded=False):
        max_sections   = st.number_input("Max sections to summarize", 5, 100, max_sections, 1)
        per_section_max_len = st.slider("Per-section max length", 80, 300, per_section_max_len, 10)
        per_section_min_len = st.slider("Per-section min length", 30, 200, per_section_min_len, 10)
        final_max_len  = st.slider("Final summary max length", 150, 500, final_max_len, 10)
        final_min_len  = st.slider("Final summary min length", 80, 300, final_min_len, 10)

    st.divider()
    if st.button("â™»ï¸ Reset app cache"):
        try: st.cache_data.clear()
        except Exception: pass
        try: st.cache_resource.clear()
        except Exception: pass
        st.success("Cache cleared. Rerun the app.")

    # ä¸å† import torchï¼ˆé¿å…è§¦å‘æŠ¥é”™ï¼‰ï¼›åªæ˜¾ç¤º Python ç‰ˆæœ¬
    st.caption(f"Python: {sys.version.split()[0]}")

if code != REAL_CODE:
    st.warning("è¯·è¾“å…¥æœ‰æ•ˆçš„è®¿é—®ç ç»§ç»­ä½¿ç”¨ã€‚")
    st.stop()

# ----------------- æ¨¡å‹ï¼ˆæ‡’åŠ è½½ï¼‰ -----------------
@st.cache_resource(show_spinner=True)
def load_summarizer_and_tokenizer():
    tok = AutoTokenizer.from_pretrained("t5-small", use_fast=True)
    # ä»…åœ¨è¿™é‡Œå»¶è¿Ÿå¯¼å…¥ torchï¼Œå¹¶é™åˆ¶çº¿ç¨‹ï¼Œç»§ç»­è§„é¿ JIT/å¹¶å‘é—®é¢˜
    try:
        import torch
        torch.set_num_threads(1)
    except Exception:
        pass
    summarizer = pipeline(
        "summarization",
        model="t5-small",
        tokenizer=tok,
        framework="pt",   # ç”¨ PyTorchï¼Œä½†å·²å¼ºåˆ¶ CPU + å…³é—­ JIT
        device=-1,
    )
    return summarizer, tok

# ----------------- Token çº§åˆ†å— -----------------
def chunk_by_tokens(tokenizer: AutoTokenizer, text: str, max_tokens: int = 480, overlap: int = 40) -> List[str]:
    if not text.strip():
        return []
    paras = [p.strip() for p in text.replace("\r\n", "\n").split("\n") if p.strip()]
    chunks: List[str] = []
    buf: List[str] = []
    buf_tokens = 0

    def tok_len(t: str) -> int:
        return len(tokenizer.encode(t, add_special_tokens=False))

    for p in paras:
        p_tokens = tok_len(p)
        if p_tokens > max_tokens:
            sentences = []
            tmp = []
            for seg in p.replace("ã€‚", "ã€‚|").replace("ï¼", "ï¼|").replace("ï¼Ÿ", "ï¼Ÿ|").split("|"):
                s = seg.strip()
                if s:
                    tmp.append(s)
                    if s[-1:] in "ã€‚ï¼ï¼Ÿ.!?":
                        sentences.append("".join(tmp)); tmp = []
            if tmp: sentences.append("".join(tmp))
            for s in sentences:
                s_tokens = tok_len(s)
                if buf_tokens + s_tokens <= max_tokens:
                    buf.append(s); buf_tokens += s_tokens
                else:
                    if buf:
                        piece = " ".join(buf); chunks.append(piece)
                        tail = piece[-overlap * 2 :] if overlap > 0 else ""
                        buf = [tail] if tail else []; buf_tokens = tok_len(" ".join(buf)) if buf else 0
                    if s_tokens <= max_tokens:
                        buf.append(s); buf_tokens = tok_len(" ".join(buf))
                    else:
                        ids = tokenizer.encode(s, add_special_tokens=False)
                        for i in range(0, len(ids), max_tokens):
                            chunks.append(tokenizer.decode(ids[i : i + max_tokens]))
                        buf, buf_tokens = [], 0
        else:
            if buf_tokens + p_tokens <= max_tokens:
                buf.append(p); buf_tokens += p_tokens
            else:
                piece = " ".join(buf); chunks.append(piece)
                tail = piece[-overlap * 2 :] if overlap > 0 else ""
                buf = [tail, p] if tail else [p]
                buf_tokens = tok_len(" ".join(buf))
    if buf:
        chunks.append(" ".join(buf))
    return [c.strip() for c in chunks if c.strip()]

# ----------------- æ–‡ä»¶ä¸Šä¼  -----------------
def main():
    uploaded = st.file_uploader("ğŸ“„ Upload a PDF file (book, report, or notes)", type="pdf")
    if not uploaded:
        return

    # ----------------- PDF è§£æ -----------------
    st.info("âœ… File uploaded successfully. Extracting textâ€¦")
    text_parts: List[str] = []
    try:
        raw = uploaded.read()
        with pdfplumber.open(io.BytesIO(raw)) as pdf:
            total_pages = len(pdf.pages)
            st.write(f"Total pages detected: **{total_pages}**")
            progress_pages = st.progress(0.0)
            for i, page in enumerate(pdf.pages, start=1):
                try:
                    t = page.extract_text(x_tolerance=2, y_tolerance=2)
                except Exception:
                    t = ""
                if t:
                    text_parts.append(t)
                if i % 10 == 0 or i == total_pages:
                    progress_pages.progress(i / total_pages)
    except Exception as e:
        st.error(f"âŒ Failed to parse PDF: {e}")
        return

    full_text = "\n".join(text_parts).strip()
    if not full_text:
        st.error("âŒ No readable text found in PDF. It may be scanned images.")
        return

    # ----------------- åˆ†å—ä¸æ‘˜è¦ -----------------
    summarizer, tokenizer = load_summarizer_and_tokenizer()
    token_chunks = chunk_by_tokens(tokenizer, full_text, max_tokens=480, overlap=40)
    st.write(f"ğŸ” Split into **{len(token_chunks)}** sections for summarization.")

    token_chunks = token_chunks[: int(max_sections)]
    progress = st.progress(0.0)
    chapter_summaries: List[str] = []

    for i, chunk in enumerate(token_chunks, start=1):
        inp = "summarize: " + chunk
        result = summarizer(
            inp,
            max_length=int(per_section_max_len),
            min_length=int(per_section_min_len),
            do_sample=False,
            clean_up_tokenization_spaces=True,
        )
        chapter_summary = result[0]["summary_text"].strip()
        chapter_summaries.append(f"### ğŸ“– Chapter {i}\n{chapter_summary}")
        progress.progress(i / len(token_chunks))

    # ----------------- è¾“å‡ºç« èŠ‚æ‘˜è¦ -----------------
    st.success("âœ… Chapter Summaries Generated!")
    for ch in chapter_summaries:
        st.markdown(ch)

    # ----------------- å…¨ä¹¦ç»¼åˆæ‘˜è¦ -----------------
    st.divider()
    st.subheader("ğŸ“™ Final Book Summary")
    combined = " ".join([s.replace("### ğŸ“– Chapter", "Chapter") for s in chapter_summaries])
    final = summarizer(
        "summarize: " + combined[:12000],
        max_length=int(final_max_len),
        min_length=int(final_min_len),
        do_sample=False,
        clean_up_tokenization_spaces=True,
    )[0]["summary_text"].strip()
    st.write(final)

    st.caption("ğŸš€ Powered by T5-small â€¢ Token-aware chunking â€¢ Optimized for long PDFs")

# â€”â€” æ•è·é¡¶å±‚å¼‚å¸¸å¹¶åœ¨é¡µé¢æ˜¾ç¤ºï¼ˆé¿å…ç™½å±ï¼‰ â€”â€” #
try:
    main()
except Exception as ex:
    st.error("App crashed with an exception:")
    st.exception(ex)
